{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Debiasing word embeddings\n","\n","Word embedding are word vectors that have meaning, word vectors similar to each other will be close to each other in a vector space.\n","\n","**After completing this lab you will be to:**\n","\n","- Use and load pre-trained word vectors\n","- Measure similarity of word vectors using cosine similarity\n","- Solve word analogy probelms such as Man is to Woman as Boy is to ____ using word embeddings\n","- Reduce gender bias in word embeddings by modifying word embeddings to remove gender stereotypes, such as the association between the words *receptionist* and *female*"],"metadata":{"id":"A7lACZZxKsY2"}},{"cell_type":"markdown","source":["## <font color='darkblue'>Word embeddings</font>\n","\n","Word embedding is a method used to represent words as vectors. They are populary used in machine learning and natural language processing tasks. Despite their success in downstream tasks such as cyberbullying, sentiment analysis, and question retrieval, they exhibit gender sterotypes which raises concerns because their widespread use can amplify these biases.\n","\n","\n","Word embeddings are trained on word co-occurance using a text dataset. After training, each word $w$ will be represented as a $d$-dimensional word vector $\\vec{w} \\space ϵ \\space ℝ^d $.\n","\n","#### Word embedding properties:\n","* Words with similar semantic meaning will be close to each other\n","* The difference between word embedding vectors can represent relationships between words. For example, given the analogy \"man is to King as woman is to $x$\" (denoted as $man:king :: woman:x$), by doing simple arithmetic on the embedding vectors, we find that $x = queen$ is the best answer because $\\vec{man} - \\vec{woman} ≈ \\vec{king} - \\vec{queen}$. For the analogy $Paris:France :: Nairobi:x$, finds that $x = Kenya$. These embeddings can also amplify sexism implicit in text. For instance, $\\vec{man} - \\vec{woman} ≈ \\vec{computer \\space programmer} - \\vec{homemaker}$. The same system that produced reasonable answers to the previous examples offensively answers \"man is to computer programmer as woman is to $x$\" with $x = homemaker$.\n","\n","Run the following cell to load the required modules."],"metadata":{"id":"UTlroLtZPGwh"}},{"cell_type":"code","execution_count":101,"metadata":{"id":"LIgOC7jfKe6w","executionInfo":{"status":"ok","timestamp":1731779737090,"user_tz":360,"elapsed":202,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"outputs":[],"source":["import os\n","import json\n","import numpy as np\n","from pathlib import Path\n","from sklearn.decomposition import PCA"]},{"cell_type":"markdown","source":["## <font color='darkblue'>Download and Load word vectors</font>\n","Due to the computational resources required to train word embeddings, we will be using a pre-trained 50-dimensional word embeddings, GloVe to represent words.\n","\n","Run the following cells to download and load the word embeddings."],"metadata":{"id":"5jgtMxNajovM"}},{"cell_type":"code","source":["def download_glove_vectors():\n","    '''\n","    Download the GloVe vectors\n","    Arguments:\n","        None\n","    Returns:\n","        file_name (String): The absolute path of the downloaded 50-dimensional\n","        GloVe word vector representations\n","    '''\n","\n","    if not Path('data').is_dir():\n","        print(\"Downloading the embeddings ...\")\n","        !wget --quiet https://nlp.stanford.edu/data/glove.6B.zip\n","        print(\"Embeddings downloaded.\")\n","\n","        # Unzip it\n","        print(\"Unzipping the downloaded file ...\")\n","        !unzip -q glove.6B.zip -d data/\n","        print(\"File unzipped.\")\n","\n","    return '/content/data/glove.6B.50d.txt'"],"metadata":{"id":"rP6R3VgXmMwQ","executionInfo":{"status":"ok","timestamp":1731779737646,"user_tz":360,"elapsed":6,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["def get_glove_vectors(glove_file):\n","    '''\n","    Read the word vectors in glove_file\n","    Arguments:\n","        glove_file (String): The absolute path to the downloaded glove word embeddings\n","    Returns:\n","        words (Set): The words (vocabulary) in the pretrained glove word embeddings\n","        word_to_vector_map (Dict): A dictionary mapping the each word to its embedding vector\n","    '''\n","\n","    words = set()\n","    word_to_vector_map = {}\n","    with open(glove_file, 'r') as file_handle:\n","        for line in file_handle:\n","            line = line.strip().split()\n","            current_word = line[0]\n","            words.add(current_word)\n","            current_word_vector = line[1:]\n","            word_to_vector_map[current_word] = np.array(current_word_vector, dtype=np.float64)\n","\n","    return words, word_to_vector_map"],"metadata":{"id":"NdjBx4rsjgTp","executionInfo":{"status":"ok","timestamp":1731779737646,"user_tz":360,"elapsed":4,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["# Load sets of words in the vocabulary and a dictionary mapping words to their GloVe vectors\n","words, word_to_vector_map = get_glove_vectors(download_glove_vectors())"],"metadata":{"id":"y0iu6d7On-Ei","executionInfo":{"status":"ok","timestamp":1731779744749,"user_tz":360,"elapsed":7106,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":["## <font color='darkblue'>Operations on word embeddings</font>\n","\n","### Task 1 - Cosine similarity\n","\n","Similarity between two words represented as word vectors $u$ and $v$ can be measured by their cosine similarity:\n","\n","$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n","\n","Where:\n","\n","$u \\cdot v$ is the dot (inner) product of the two vectors\n","\n","$||u||_2$ is the length of the vector $u$. The length also called Euclidean length or Euclidean norm defines a distance function defined as $||u||_2 = \\sqrt{u_1^2 \\space + \\space ... \\space + \\space u_n^2}$\n","\n","The normalized similarity between $u$ and $v$ is the cosine of the angle between the two vectors denoted as $\\theta$. The cosine similarity of $u$ and $v$ will be close to 1 if the two vectors are similar, otherwise, the cosine similarity will be small.\n","\n","**Note**: We will be refering to the embedding of a word i.e the word vector and the word interchangeably in this lab.\n","\n","\n","***\n","**<font color='red'>Task 1a:</font>** Implement equation 1 in the `cosine_similarity()` function below. <br> Hint: check out the numpy documentation on [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html), and [np.sqrt](https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html). Depending on how you choose to implement it, you can check out [np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n","***\n"],"metadata":{"id":"VFF8yxWxq06z"}},{"cell_type":"code","source":["def cosine_similarity(vector1, vector2):\n","    \"\"\"\n","    Calculates the cosine similarity of two word vectors - vector1 and vector2\n","    Arguments:\n","        vector1 (ndarray): A word vector having shape (n,)\n","        vector2 (ndarray): A word vector having shape (n,)\n","    Returns:\n","        cosine_similarity (float): The cosine similarity between vector1 and vector2\n","    \"\"\"\n","\n","    # Compute the dot product between vector1 and vector2\n","    dot = np.dot(vector1, vector2)\n","\n","    # Compute the Euclidean norm or length of vector1\n","    norm_vector1 = np.linalg.norm(vector1)\n","\n","    # Compute the Euclidean norm or length of vector2\n","    norm_vector2 = np.linalg.norm(vector2)\n","\n","    # Compute the cosine similarity as defined in equation 1\n","    cosine_similarity = dot / (norm_vector1 * norm_vector2)\n","\n","    return cosine_similarity"],"metadata":{"id":"NfhVLS8Ow8gA","executionInfo":{"status":"ok","timestamp":1731779744974,"user_tz":360,"elapsed":227,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["# Run this cell to obtain and report your answers\n","man = word_to_vector_map[\"man\"]\n","woman = word_to_vector_map[\"woman\"]\n","cat = word_to_vector_map[\"cat\"]\n","dog = word_to_vector_map[\"dog\"]\n","orange = word_to_vector_map[\"orange\"]\n","england = word_to_vector_map[\"england\"]\n","london = word_to_vector_map[\"london\"]\n","edinburgh = word_to_vector_map[\"edinburgh\"]\n","scotland = word_to_vector_map[\"scotland\"]\n","\n","print(f\"Cosine similarity between man and woman: {cosine_similarity(man, woman)}\")\n","print(f\"Cosine similarity between cat and dog: {cosine_similarity(cat, dog)}\")\n","print(f\"Cosine similarity between cat and cow: {cosine_similarity(cat, orange)}\")\n","print(f\"Cosine similarity between england - london and edinburgh - scotland: {cosine_similarity(england - london, edinburgh - scotland)}\")"],"metadata":{"id":"eP-Kxp7YBcDw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779744974,"user_tz":360,"elapsed":6,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"c4bd8cef-49f7-4de0-8ea6-08355ebb5c8b"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between man and woman: 0.886033771849582\n","Cosine similarity between cat and dog: 0.9218005273769252\n","Cosine similarity between cat and cow: 0.40695688711826294\n","Cosine similarity between england - london and edinburgh - scotland: -0.5203389719861108\n"]}]},{"cell_type":"markdown","source":["**<font color='red'>Task 1b:</font>** In the code cell below, try out 3 of your own inputs here and report your inputs and outputs"],"metadata":{"id":"2FMC1hZ4njIH"}},{"cell_type":"code","source":["# Run this cell to obtain and report your answers\n","king = word_to_vector_map[\"king\"]\n","queen = word_to_vector_map[\"queen\"]\n","tiger = word_to_vector_map[\"tiger\"]\n","lion = word_to_vector_map[\"lion\"]\n","apple = word_to_vector_map[\"apple\"]\n","france = word_to_vector_map[\"france\"]\n","paris = word_to_vector_map[\"paris\"]\n","berlin = word_to_vector_map[\"berlin\"]\n","germany = word_to_vector_map[\"germany\"]\n","\n","print(f\"Cosine similarity between king and queen: {cosine_similarity(king, queen)}\")\n","print(f\"Cosine similarity between tiger and lion: {cosine_similarity(tiger, lion)}\")\n","print(f\"Cosine similarity between tiger and apple: {cosine_similarity(tiger, apple)}\")\n","print(f\"Cosine similarity between france - paris and berlin - germany: {cosine_similarity(france - paris, berlin - germany)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxiE6sIBsfy2","executionInfo":{"status":"ok","timestamp":1731779744974,"user_tz":360,"elapsed":5,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"12f4d57c-0d0e-4d06-e8f5-782071bf5a92"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between king and queen: 0.7839043010964117\n","Cosine similarity between tiger and lion: 0.5493991225945609\n","Cosine similarity between tiger and apple: 0.36045373632604055\n","Cosine similarity between france - paris and berlin - germany: -0.795694872209922\n"]}]},{"cell_type":"markdown","source":["### Task 2 - Word analogy\n","\n","In an analogy task, you are given an analogy in the form \"i is to j as k is to ___\". Your task is to complete this sentence.\n","\n","For example, if you are given \"man is to king as woman is to $l$\" (denoted as $man:king :: woman:l$). You are to find the best word $l$ that answers the analogy the best. Simple arithmetic of the embedding vectors will find that $l = queen$ is the best answer because the embedding vectors of words $i$, $j$, $k$, and $l$ denoted as $e_i$, $e_j$, $e_k$, $e_l$ have the following relationship:\n","$$e_j - e_i ≈ e_l - e_k$$\n","\n","Cosine similarity can be used to measure the similarity between $e_j - e_i$ and $e_l - e_k$\n","\n","***\n","**<font color='red'>Task 2a:</font>** To perform word analogies, implement `answer_analogy()` below.\n","***\n"],"metadata":{"id":"LmsetI1rFUcy"}},{"cell_type":"code","source":["# Fixed the issue this code now works Nov 14, 2024 ECruz\n","\n","def answer_analogy(word_i, word_j, word_k, word_to_vector_map):\n","    \"\"\"\n","    Performs word analogy as described above\n","    Arguments:\n","        word_i (String): A word\n","        word_j (String): A word\n","        word_k (String): A word\n","        word_to_vector_map (Dict): A dictionary of words as key and its associated embedding vector as value\n","    Returns:\n","        best_word (String): A word that fufils the relationship that e_j - e_i as close as possible to e_l - e_k, as measured by cosine similarity\n","    \"\"\"\n","\n","    # Convert words to lowercase\n","    word_i = word_i.lower()\n","    word_j = word_j.lower()\n","    word_k = word_k.lower()\n","\n","    # Start code here #\n","    try:\n","        # Get the embedding vectors of word_i (~ 1 line)\n","        embedding_vector_of_word_i = word_to_vector_map[word_i] # Fetch embedding from word_to_vector_map\n","    except KeyError:\n","        print(f\"{word_i} is not in our vocabulary. Please try a different word.\")\n","        return\n","\n","    try:\n","        # Get the embedding vectors of word_j (~ 1 line)\n","        embedding_vector_of_word_j = word_to_vector_map[word_j] # Fetch embedding from word_to_vector_map\n","    except KeyError:\n","        print(f\"{word_j} is not in our vocabulary. Please try a different word.\")\n","        return\n","\n","    try:\n","        # Get the embedding vectors of word_k (~ 1 line)\n","        embedding_vector_of_word_k = word_to_vector_map[word_k] # Fetch embedding from word_to_vector_map\n","    except KeyError:\n","        print(f\"{word_k} is not in our vocabulary. Please try a different word.\")\n","        return\n","    # End code here #\n","\n","    # Get all the words in our word to vector map i.e our vocabulary\n","    words = word_to_vector_map.keys()\n","    max_cosine_similarity = -1000                           # Initialize to a large negative number\n","    best_word = None                                        # Note: Do not change this None. Keeps track of the word that best answers the analogy.\n","\n","    # Since we are looping through the whole vocabulary, if we encounter a word\n","    # that is the same as our input, that word becomes the best_word. To avoid\n","    # that we skip the input word.\n","    input_words = set([word_i, word_j, word_k])\n","\n","    for word in words:\n","        if word in input_words:\n","            continue\n","\n","        # Start code here #\n","        # Compute cosine similarity  (~ 1 line)\n","        # Calculate cosine similarity using embedding vectors\n","        embedding_vector_of_word_l = word_to_vector_map[word]\n","        a = embedding_vector_of_word_j - embedding_vector_of_word_i\n","        b = embedding_vector_of_word_l - embedding_vector_of_word_k\n","        similarity = np.dot"],"metadata":{"id":"q5EspY1Pxrqj","executionInfo":{"status":"ok","timestamp":1731779744974,"user_tz":360,"elapsed":3,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 2b:</font>** Test your implementation by running the code cell below. What are your observations? What do you observe about the last two outputs?.\n","***\n"],"metadata":{"id":"PzFBmLKKyDD8"}},{"cell_type":"code","source":["analogies = [('france', 'french', 'germany'),\n","             ('england', 'london', 'japan'),\n","             ('boy', 'girl', 'man'),\n","             ('man', 'doctor', 'woman'),\n","             ('small', 'smaller', 'big')]\n","for analogy in analogies:\n","    best_word = answer_analogy(*analogy, word_to_vector_map)\n","    if best_word:\n","        print(f\"{analogy[0]} -> {analogy[1]} :: {analogy[2]} -> {best_word}\")"],"metadata":{"id":"wSB0BR5YvPg0","executionInfo":{"status":"ok","timestamp":1731779751222,"user_tz":360,"elapsed":6251,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 2c:</font>** Try your own analogies by completing and executing the code cell below. Find 2 that works and one that doesn't. Report your inputs and outputs\n","***"],"metadata":{"id":"yFAAQaTSw1cJ"}},{"cell_type":"code","source":["# Define your own analogies here\n","my_analogies = [\n","    ('king', 'queen', 'prince'),\n","    ('paris', 'france', 'nice'),\n","    ('car', 'road', 'sign')\n","]\n","\n","# Execute the analogies\n","for analogy in my_analogies:\n","    best_word = answer_analogy(*analogy, word_to_vector_map)\n","    print(f\"{analogy[0]} -> {analogy[1]} :: {analogy[2]} -> {best_word}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kq6r7ZPly74z","executionInfo":{"status":"ok","timestamp":1731779753839,"user_tz":360,"elapsed":2620,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"17250c0a-b4f4-45c7-8128-5b6d69a26076"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["king -> queen :: prince -> None\n","paris -> france :: nice -> None\n","car -> road :: sign -> None\n"]}]},{"cell_type":"markdown","source":["### Task 3 - Geometry of Gender and Bias in Word Embeddings: Occupational stereotypes\n","In this task, we will understand the biases present in word-embedding i.e which words are closer to $she$ than to $he$. This will be achieved by evaluating whether the GloVe embeddings have sterotypes on occupation words. Determine gender bias by projecting each of the occupations onto the $she - he$ direction by computing the dot product between each occupation word embedding and the embedding vector of $she - he$ normalized by the Euclidean norm (See task 1).\n","\n","$$occupation\\_word_i \\cdot ||she - he||_2 \\tag{2}$$\n","\n","Notice that equation 2 is similar to only the numerator of equation 1 because we are computing the dot product of $occupation\\_word_i$ and the normalized difference between $she$ and $he$.\n","\n","Run the cells below to download and view the occupations."],"metadata":{"id":"3sbFypomLRbE"}},{"cell_type":"code","source":["def download_occupations():\n","    if not Path('debiaswe').is_dir():\n","        print(\"Downloading occupation list ...\")\n","        !git clone -q https://github.com/tolga-b/debiaswe.git\n","        print(\"Occupation list downloaded.\")\n","\n","    return '/content/debiaswe/data/professions.json'\n","\n","\n","def view_occupations(occupations_file):\n","    with open(occupations_file, 'r') as file_handle:\n","        occupations = json.load(file_handle)\n","\n","        for occupation in occupations:\n","            print(occupation[0])"],"metadata":{"id":"Rg5hB08IX8PB","executionInfo":{"status":"ok","timestamp":1731779753839,"user_tz":360,"elapsed":5,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["occupations_file = download_occupations()"],"metadata":{"id":"gpWAw2r4oK59","executionInfo":{"status":"ok","timestamp":1731779753839,"user_tz":360,"elapsed":5,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["view_occupations(occupations_file)"],"metadata":{"id":"O1dlznxmliBJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779753839,"user_tz":360,"elapsed":4,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"4333c261-3c64-4f82-964a-e1978778da38"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["accountant\n","acquaintance\n","actor\n","actress\n","adjunct_professor\n","administrator\n","adventurer\n","advocate\n","aide\n","alderman\n","alter_ego\n","ambassador\n","analyst\n","anthropologist\n","archaeologist\n","archbishop\n","architect\n","artist\n","artiste\n","assassin\n","assistant_professor\n","associate_dean\n","associate_professor\n","astronaut\n","astronomer\n","athlete\n","athletic_director\n","attorney\n","author\n","baker\n","ballerina\n","ballplayer\n","banker\n","barber\n","baron\n","barrister\n","bartender\n","biologist\n","bishop\n","bodyguard\n","bookkeeper\n","boss\n","boxer\n","broadcaster\n","broker\n","bureaucrat\n","businessman\n","businesswoman\n","butcher\n","butler\n","cab_driver\n","cabbie\n","cameraman\n","campaigner\n","captain\n","cardiologist\n","caretaker\n","carpenter\n","cartoonist\n","cellist\n","chancellor\n","chaplain\n","character\n","chef\n","chemist\n","choreographer\n","cinematographer\n","citizen\n","civil_servant\n","cleric\n","clerk\n","coach\n","collector\n","colonel\n","columnist\n","comedian\n","comic\n","commander\n","commentator\n","commissioner\n","composer\n","conductor\n","confesses\n","congressman\n","constable\n","consultant\n","cop\n","correspondent\n","councilman\n","councilor\n","counselor\n","critic\n","crooner\n","crusader\n","curator\n","custodian\n","dad\n","dancer\n","dean\n","dentist\n","deputy\n","dermatologist\n","detective\n","diplomat\n","director\n","disc_jockey\n","doctor\n","doctoral_student\n","drug_addict\n","drummer\n","economics_professor\n","economist\n","editor\n","educator\n","electrician\n","employee\n","entertainer\n","entrepreneur\n","environmentalist\n","envoy\n","epidemiologist\n","evangelist\n","farmer\n","fashion_designer\n","fighter_pilot\n","filmmaker\n","financier\n","firebrand\n","firefighter\n","fireman\n","fisherman\n","footballer\n","foreman\n","freelance_writer\n","gangster\n","gardener\n","geologist\n","goalkeeper\n","graphic_designer\n","guidance_counselor\n","guitarist\n","hairdresser\n","handyman\n","headmaster\n","historian\n","hitman\n","homemaker\n","hooker\n","housekeeper\n","housewife\n","illustrator\n","industrialist\n","infielder\n","inspector\n","instructor\n","interior_designer\n","inventor\n","investigator\n","investment_banker\n","janitor\n","jeweler\n","journalist\n","judge\n","jurist\n","laborer\n","landlord\n","lawmaker\n","lawyer\n","lecturer\n","legislator\n","librarian\n","lieutenant\n","lifeguard\n","lyricist\n","maestro\n","magician\n","magistrate\n","maid\n","major_leaguer\n","manager\n","marksman\n","marshal\n","mathematician\n","mechanic\n","mediator\n","medic\n","midfielder\n","minister\n","missionary\n","mobster\n","monk\n","musician\n","nanny\n","narrator\n","naturalist\n","negotiator\n","neurologist\n","neurosurgeon\n","novelist\n","nun\n","nurse\n","observer\n","officer\n","organist\n","painter\n","paralegal\n","parishioner\n","parliamentarian\n","pastor\n","pathologist\n","patrolman\n","pediatrician\n","performer\n","pharmacist\n","philanthropist\n","philosopher\n","photographer\n","photojournalist\n","physician\n","physicist\n","pianist\n","planner\n","plastic_surgeon\n","playwright\n","plumber\n","poet\n","policeman\n","politician\n","pollster\n","preacher\n","president\n","priest\n","principal\n","prisoner\n","professor\n","professor_emeritus\n","programmer\n","promoter\n","proprietor\n","prosecutor\n","protagonist\n","protege\n","protester\n","provost\n","psychiatrist\n","psychologist\n","publicist\n","pundit\n","rabbi\n","radiologist\n","ranger\n","realtor\n","receptionist\n","registered_nurse\n","researcher\n","restaurateur\n","sailor\n","saint\n","salesman\n","saxophonist\n","scholar\n","scientist\n","screenwriter\n","sculptor\n","secretary\n","senator\n","sergeant\n","servant\n","serviceman\n","sheriff_deputy\n","shopkeeper\n","singer\n","singer_songwriter\n","skipper\n","socialite\n","sociologist\n","soft_spoken\n","soldier\n","solicitor\n","solicitor_general\n","soloist\n","sportsman\n","sportswriter\n","statesman\n","steward\n","stockbroker\n","strategist\n","student\n","stylist\n","substitute\n","superintendent\n","surgeon\n","surveyor\n","swimmer\n","taxi_driver\n","teacher\n","technician\n","teenager\n","therapist\n","trader\n","treasurer\n","trooper\n","trucker\n","trumpeter\n","tutor\n","tycoon\n","undersecretary\n","understudy\n","valedictorian\n","vice_chancellor\n","violinist\n","vocalist\n","waiter\n","waitress\n","warden\n","warrior\n","welder\n","worker\n","wrestler\n","writer\n"]}]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 3a:</font>** Complete the `get_occupation_stereotypes()` below.\n","***"],"metadata":{"id":"Ci7j7szxzF53"}},{"cell_type":"code","source":["#completed code Edward Cruz\n","\n","def get_occupation_stereotypes(she, he, occupations_file, word_to_vector_map, verbose=False):\n","    \"\"\"\n","    Computes the words that are closest to she and he in the GloVe embeddings\n","    Arguments:\n","        she (String): A word\n","        he (String): A word\n","        occupations_file (String): The path to the occupation file\n","        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n","    Returns:\n","        most_similar_words (Tuple(List[Tuple(Float, String)], List[Tuple(Float, String)])):\n","        A tuple of the list of the most similar occupation words to she and he with their associated similarity\n","    \"\"\"\n","\n","    # Read occupations\n","    with open(occupations_file, 'r') as file_handle:\n","        occupations = json.load(file_handle)\n","\n","    # Extract occupation words\n","    occupation_words = [occupation[0] for occupation in occupations]\n","\n","    # Get embedding vector of she\n","    embedding_vector_she = word_to_vector_map[she]\n","    # Get embedding vector of he\n","    embedding_vector_he = word_to_vector_map[he]\n","    # Get the vector difference between embedding vectors of she and he\n","    vector_difference_she_he = embedding_vector_she - embedding_vector_he\n","    # Get the normalized difference\n","    normalized_difference_she_he = vector_difference_she_he / np.linalg.norm(vector_difference_she_he)\n","\n","    # Store the cosine similarities\n","    similarities = []\n","\n","    for word in occupation_words:\n","        try:\n","            # Get the embedding vector of the current occupation word\n","            occupation_word_embedding_vector = word_to_vector_map[word]\n","            # Compute cosine similarity between embedding vector of the occupation word and normalized she - he vector\n","            similarity = np.dot(occupation_word_embedding_vector, normalized_difference_she_he) / np.linalg.norm(occupation_word_embedding_vector)\n","            similarities.append((similarity, word))\n","        except KeyError:\n","            if verbose:\n","                print(f\"{word} is not in our vocabulary.\")\n","\n","    # Sort similarities\n","    most_similar_words = sorted(similarities)\n","\n","    return most_similar_words[:20], most_similar_words[-20:]\n"],"metadata":{"id":"nr7zafYdByDO","executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":232,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 3b:</font>** Execute the cell below and report your results.\n","\n","1) Does the GloVe word embeddings propagate bias? why?\n","\n","2) From the list associated with she, list those that reflect gender stereotype.   \n","\n","3) Compare your list from 2 to the occupations closest to he. What are your conclusions?\n","\n","Exclude businesswoman from your list.\n","***"],"metadata":{"id":"SbrKEw6NzMea"}},{"cell_type":"code","source":["he, she = get_occupation_stereotypes('she', 'he', occupations_file, word_to_vector_map)\n","\n","print(\"Occupations closest to he:\")\n","for occupation in he:\n","    print(f\"{occupation[0], occupation[1]}\")\n","\n","print(\"\\nOccupations closest to she:\")\n","for occupation in she:\n","    if occupation[1] != 'businesswoman': # Excluding businesswoman\n","        print(f\"{occupation[0], occupation[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-l22vF50IXU","executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":28,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"5380c9e9-5e66-4e7a-f6f1-bb48172986a2"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["Occupations closest to he:\n","(-0.35621238403428845, 'coach')\n","(-0.33694609670744985, 'caretaker')\n","(-0.316340865049864, 'captain')\n","(-0.30927324022117164, 'marshal')\n","(-0.30729135719953826, 'colonel')\n","(-0.3024871694158589, 'skipper')\n","(-0.3021438553042009, 'manager')\n","(-0.30165373167213716, 'midfielder')\n","(-0.2996711177716737, 'archbishop')\n","(-0.29443064216114034, 'commander')\n","(-0.2902867659336358, 'footballer')\n","(-0.2888985018624171, 'bishop')\n","(-0.2819996009929521, 'marksman')\n","(-0.2796388242138503, 'firebrand')\n","(-0.27878757562089734, 'provost')\n","(-0.2780793318566927, 'substitute')\n","(-0.27217972416807396, 'lieutenant')\n","(-0.2719793191353711, 'custodian')\n","(-0.2719191225303118, 'superintendent')\n","(-0.27134651130558013, 'goalkeeper')\n","\n","Occupations closest to she:\n","(0.32072737162261483, 'singer')\n","(0.3219568449876504, 'publicist')\n","(0.3440519945920838, 'nanny')\n","(0.34466952852982563, 'therapist')\n","(0.3474659148960079, 'confesses')\n","(0.3573048226883874, 'dancer')\n","(0.3645661876706785, 'hairdresser')\n","(0.36983542416768733, 'receptionist')\n","(0.3729106376588047, 'housekeeper')\n","(0.37309747916414077, 'homemaker')\n","(0.3812397049469023, 'housewife')\n","(0.3813398603461959, 'nurse')\n","(0.3892646060961875, 'narrator')\n","(0.4138336650968037, 'maid')\n","(0.42853139796990286, 'socialite')\n","(0.4434377286463442, 'waitress')\n","(0.44732913871667485, 'stylist')\n","(0.4686138536614218, 'ballerina')\n","(0.49840294304026295, 'actress')\n"]}]},{"cell_type":"markdown","source":["<!--\n","### Task 4 - Analogies exhibiting stereotypes\n","Using analogies to quantify gender stereotype in the embedding. Given two words, e.g. $he$, $she$, generate a pair of words, $x$ and $y$, such that $he$ to $x$ as $she$ to $y$ is a good analogy. This will generate pairs that the embedding believes to be analogous to $he$, $she$ or any other pair of seed words.\n","\n","The analogy generator takes as input a pair of seed words (a, b) which determines the seed direction $\\vec{a} - \\vec{b}$ corresponding to the ***normalized*** difference between the two seed words.\n","\n","In this task, we will use $(a, b) = (she, he)$. Then all pairs of words $x, y$ is scored using the following metric:\n","\n","$$S_{(a, b)}(x, y) = ||\\vec{a} - \\vec{b}||_2 \\cdot ||\\vec{x} - \\vec{y}||_2 \\space \\text{if} \\space ||\\vec{x} - \\vec{y}||_2 ≤ δ, 0 \\space \\text{else}$$\n","\n","Where $δ$ is a threshold for semantic similarity. We will use $δ = 1$.\n","\n","<!-- In other words, the above equation reads, if the normalized difference between $x$ and $y$ is less than or equal to our threshold, then the score of the pair of words $x, y$ is the dot product of the normalized difference between the seed pairs and the normalized difference between the pair of words $x, y$. ->\n","\n","Notice that each vector difference is normalized, therefore we are basically computing the numerator of equation 1 as part of this equation.\n","\n","***\n","**<font color='red'>Task 4:</font>** Test the implementation of your `get_analogies_exhibiting_stereotypes()` below. Report your results. Are the generated analogies biased?\n","***\n","-->\n","\n","### Task 4 - Debiasing word embeddings\n","\n","**Gender Specific words**\n","\n","Words that are associated with a gender by definition. For example, brother, sister, businesswoman or businessman.\n","\n","**Gender neutral words**\n","\n","The remanining words that are not specific to a gender are gender neutral. For example, flight attendant or shoes. The compliment of gender specific words, can be taken as the gender neutral words.\n","\n","**Step 1 - Identify gender subspace i.e identify the direction of the embedding that captures the bias**\n","\n","To robustly estimate bias, we use the gender specific words to learn a gender subpace in the embedding. To identify the gender subspace, we consider the vector difference of gender specific word pairs, such as $\\vec{she} - \\vec{he}$, $\\vec{woman} - \\vec{man}$ or $\\vec{her} - \\vec{his}$. This identifies a **gender direction or bias subspace** $g \\space ϵ \\space ℝ^d$ which captures gender in the embedding.\n","\n","**Note:** We will use $g$ and $bias\\_direction$ interchangeably in this lab."],"metadata":{"id":"w4dyeTcJxqdA"}},{"cell_type":"code","source":["gender = word_to_vector_map['she'] - word_to_vector_map['he']\n","print(gender)"],"metadata":{"id":"1w_fti_WxzCZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":27,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"92878316-f420-4991-de64-c1709a76807e"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.261302   0.438481  -0.13376    0.12281    0.00838    0.64455\n","  0.13151    0.01198    0.73557   -0.04754   -0.04261   -0.23387\n","  0.56951    0.24359    0.29471    0.152461  -0.44638    0.08563\n","  0.66735   -0.202578   0.28133    0.71557    0.04015    0.42204\n","  0.63574    0.1193    -0.429694   0.216301   0.08826   -0.5115\n"," -0.286      0.227249   0.25811    0.18075   -0.22733   -0.151844\n"," -0.13196   -0.14412    0.01709   -0.6281     0.124465  -0.16902\n","  0.6244667 -0.53734    0.379254  -0.3373     0.384876  -0.92383\n"," -0.019064   0.435641 ]\n"]}]},{"cell_type":"markdown","source":["The gender subspace can also be captured more accurately by taking gender pair difference vectors and computing its principal components (PCs). The top PC, denoted by the unit vector $g$, captures the gender subspace."],"metadata":{"id":"wrsBGsZPcEte"}},{"cell_type":"code","source":["def get_gender_subspace(pairs, word_to_vector_map, num_components=10):\n","    \"\"\"\n","    Compute the gender subspace by computing the principal components of\n","    ten gender pair vectors.\n","    Arguments:\n","        pairs (List[Tuple(String, String)]): A list of gender specific word pairs\n","        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n","        num_components (Int): The number of principal components to compute. Defaults to 10\n","    Returns:\n","        gender_subspace (ndarray): The gender bias subspace(or direction) of shape (embedding dimension,)\n","    \"\"\"\n","\n","    matrix = []\n","    for word_1, word_2 in pairs:\n","        embedding_vector_word_1 = word_to_vector_map[word_1]\n","        embedding_vector_word_2 = word_to_vector_map[word_2]\n","        center = (embedding_vector_word_1 + embedding_vector_word_2) / 2\n","        matrix.append(embedding_vector_word_1 - center)\n","        matrix.append(embedding_vector_word_2 - center)\n","\n","    matrix = np.array(matrix)\n","    pca = PCA(n_components=num_components)\n","    pca.fit(matrix)\n","\n","    pcs = pca.components_                  # Sorted by decreasing explained variance\n","    eigenvalues = pca.explained_variance_  # Eigenvalues\n","    gender_subspace = pcs[0]               # The first element has the highest eigenvalue\n","    return gender_subspace"],"metadata":{"id":"C2gVI7TvcvDp","executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":25,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["gender_specific_pairs = [\n","    ('she', 'he'),\n","    ('her', 'his'),\n","    ('woman', 'man'),\n","    ('mary', 'john'),\n","    ('herself', 'himself'),\n","    ('daughter', 'son'),\n","    ('mother', 'father'),\n","    ('gal', 'guy'),\n","    ('girl', 'boy'),\n","    ('female', 'male')\n","]\n","gender_direction = get_gender_subspace(gender_specific_pairs, word_to_vector_map)\n","print(gender_direction)"],"metadata":{"id":"3qsDDbxTgYSO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":24,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"4698ba1d-bc17-43b8-cb32-18cb9d7ae640"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.06639123  0.15316702 -0.12170385  0.02910502 -0.012115    0.2956192\n","  0.10015248  0.03503806  0.27605339 -0.06259264  0.04843718 -0.20243709\n","  0.22435017  0.02205075  0.08795604  0.05350635 -0.23457441 -0.0051648\n","  0.29096997  0.02894429  0.10423079  0.24379617  0.05296573  0.17222571\n","  0.13557158  0.13746521 -0.05081975  0.11252051  0.01639264 -0.2113686\n"," -0.1403471   0.13498117  0.08092433  0.02423979 -0.10780551 -0.05927322\n"," -0.04857578 -0.03199024  0.08174041 -0.17759707 -0.02782478 -0.16880811\n","  0.27589146 -0.18007478  0.04123208 -0.09385728  0.11011447 -0.25650007\n","  0.06258361  0.00847159]\n"]}]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 4a:</font>** Run the cell below to computes the similarity between the gender embedding and the embedding vectors of male and female names. What can you observe?\n","***"],"metadata":{"id":"2dqlFhLsXhsA"}},{"cell_type":"code","source":["print('Names and their similarities with simple gender subspace')\n","names = [\"mary\", \"john\", \"sweta\", \"david\", \"kazim\", \"angela\"]\n","for name in names:\n","    print(name, cosine_similarity(word_to_vector_map[name], gender))\n","\n","print()\n","print('Names and their similarities with PCA based gender subspace')\n","names = [\"mary\", \"john\", \"sweta\", \"david\", \"kazim\", \"angela\"]\n","for name in names:\n","    print(name, cosine_similarity(word_to_vector_map[name], gender_direction))"],"metadata":{"id":"V0dQ7vU6XRqs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":21,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"fe5b55d1-79ed-42a1-dd0b-51dd5e567034"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["Names and their similarities with simple gender subspace\n","mary 0.3457399102816379\n","john -0.17879783833420468\n","sweta 0.17016456601128147\n","david -0.1332261560078667\n","kazim -0.32658964009764835\n","angela 0.2600799146632235\n","\n","Names and their similarities with PCA based gender subspace\n","mary 0.2637091204419718\n","john -0.3816839789078354\n","sweta 0.1773704777691709\n","david -0.3165647635266187\n","kazim -0.3249838182709315\n","angela 0.18623308926276097\n"]}]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 4b:</font>** Quantify direct and indirect biases between words and the gender embedding by running the following cell. What is your observation?\n","***"],"metadata":{"id":"7-3f7NiKZHRB"}},{"cell_type":"code","source":["words = [\"engineer\", \"science\", \"pilot\", \"technology\", \"lipstick\", \"arts\", \"singer\", \"computer\", \"receptionist\", \"fashion\", \"doctor\", \"literature\"]\n","for word in words:\n","    print(word, cosine_similarity(word_to_vector_map[word], gender_direction))"],"metadata":{"id":"v9b4NpyZZR-K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779754068,"user_tz":360,"elapsed":18,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"70ee126f-849b-4fca-9d9a-10fc5057ced9"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["engineer -0.2626286258749398\n","science -0.1202780958734538\n","pilot -0.1319833052724868\n","technology -0.1801116607819377\n","lipstick 0.4179404715417419\n","arts -0.04513818522820779\n","singer 0.16162975755073875\n","computer -0.16390549337211754\n","receptionist 0.3305284235998437\n","fashion 0.06913524872078784\n","doctor 0.02885191966409418\n","literature -0.08972688088254833\n"]}]},{"cell_type":"markdown","source":["**Step 2 - Neutralize gender neutral words**\n","\n","Ensures that gender neutral words are zero in the gender subspace. This means that this steps takes a vector such as $e_{fashion}$ and turns its components into zeros in the direction of $g$ to produce $e_{fashion}^{debiased}$\n","\n","To remove bias in words such as \"receptionist\" or \"shoe\", given an input embedding of the word $e$, we compute debiased $e$ denoted as $e^{debiased}$ by using the formulas:\n","\n","$$e^{bias\\_component} = \\frac{e \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction\\tag{3}$$\n","\n","$$e^{debiased} = e - e^{bias\\_component}\\tag{4}$$\n","\n","Where $e^{bias\\_component}$ is the projection of the word embedding $e$ onto the gender subspace. Since the gender subspace is an orthogonal unit vector it is simply a direction. This also means that $e^{debiased}$ is the projection onto the orthogonal subspace.\n","\n","$||g||_2^2$ is the squared euclidean norm of $g$ formulated as:\n","\n","$$||g||_2^2 = {\\sum}_i \\space g_i^2$$\n","\n","\n","***\n","**<font color='red'>Task 4c:</font>** Implement `neutralize()` below by implementing the formulas above. Hint see [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\n","***"],"metadata":{"id":"H81h64YIae53"}},{"cell_type":"code","source":["\n","def neutralize(word, gender_direction, word_to_vector_map):\n","    \"\"\"\n","    Project the vector of word onto the gender subspace to remove the bias of \"word\"\n","    Arguments:\n","        word (String): A word to debias\n","        gender_direction (ndarray): Numpy array of shape (embedding size (50), ) which is the bias axis\n","        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n","\n","    Returns:\n","        debiased_word (ndarray): the vector representation of the neutralized input word\n","    \"\"\"\n","\n","    # Get the vector representation of word\n","    embedding_of_word = word_to_vector_map[word]\n","\n","    # Compute the projection of word onto gender direction (eq. 3)\n","    projection_of_word_onto_gender = (np.dot(embedding_of_word, gender_direction) / np.sum(gender_direction**2)) * gender_direction\n","\n","    # Neutralize word (eq. 4)\n","    debiased_word  = embedding_of_word - projection_of_word_onto_gender\n","\n","    return debiased_word"],"metadata":{"id":"bLB4Z6NDCNrr","executionInfo":{"status":"ok","timestamp":1731779754069,"user_tz":360,"elapsed":17,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":["\n","***\n","**<font color='red'>Task 4d:</font>** Test your implementation by running the code cell below. What is your observation?\n","***"],"metadata":{"id":"8UqUM48Q9mGh"}},{"cell_type":"code","source":["word = \"babysit\"\n","print(f\"Before neutralization, cosine similarity between {word} and gender is: {cosine_similarity(word_to_vector_map[word], gender_direction)}\")\n","\n","debiased_word = neutralize(word, gender_direction, word_to_vector_map)\n","print(f\"After neutralization, cosine similarity between {word} and gender is: {cosine_similarity(debiased_word, gender_direction)}\")"],"metadata":{"id":"IBlXaobP8k3s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731779754069,"user_tz":360,"elapsed":16,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"33454676-877a-4856-9574-85675b60f6b0"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["Before neutralization, cosine similarity between babysit and gender is: 0.2663444879209918\n","After neutralization, cosine similarity between babysit and gender is: -1.3389570015765782e-17\n"]}]},{"cell_type":"markdown","source":["**Step 3 - Equalize**\n","\n","Equalizes sets of gender specific words outside the subspace. The goal is to ensure that gender neutral words are equidistance to all the words in the set. We want to ensure that gender specific words are not biased with respect to neutral words.\n","\n","For example, consider the set {woman, man}, if the neutral word \"babysit\" is closer to \"woman\" than \"man\" then the neutralization of \"babysit\" can reduce the gender-stereotype associated with babysitting but does not make \"babysit\" equidistant to \"woman\" and \"man\".\n","\n","Given two gender specific word pairs $w_1$ and $w_2$ to debias, and their embeddings $e_{w_1}$ and $e_{w_2}$,   equalization can be achieved with the following equations:\n","\n","$$ \\mu = \\frac{e_{w_1} + e_{w_2}}{2} \\tag{5}$$\n","\n","$$ \\mu_{B} = \\frac{\\mu \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction \\tag{6}$$\n","\n","$$ v = \\mu - \\mu_B \\tag{7}$$\n","\n","$$ e_{w_1B} = \\frac{e_{w_1} \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction \\tag{8}$$\n","\n","$$ e_{w_2B} = \\frac{e_{w_2} \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction \\tag{9}$$\n","\n","$$ e_{w_1B}^{new} = \\sqrt{|1 - ||v||_2^2|} * \\frac{e_{w_1B} - \\mu_B}{||(e_{w_1} - v) - \\mu_B||_2} \\tag{10}$$\n","\n","$$  e_{w_2B}^{new} = \\sqrt{|1 - ||v||_2^2|} * \\frac{e_{w_2B} - \\mu_B}{||(e_{w_2} - v) - \\mu_B||_2} \\tag{11}$$\n","\n","$$ e_1 = v + e_{w_1B}^{new} \\tag{12}$$\n","\n","$$ e_2 = v + e_{w_2B}^{new} \\tag{13}$$"],"metadata":{"id":"Tph-OAkfh9O2"}},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 5a:</font>** Implement `equalization()` below by implementing the formulas above.\n","***"],"metadata":{"id":"CihNFJFY0gBt"}},{"cell_type":"code","source":["def equalization(equality_set, bias_direction, word_to_vector_map):\n","    \"\"\"\n","    Equalize the pair of gender specific words in the equality set ensuring that\n","    any neutral word is equidistant to all words in the equality set.\n","    Arguments:\n","        equality_set (Tuple(String, String)): a tuple of strings of gender specific\n","        words to debias e.g (\"grandmother\", \"grandfather\")\n","        bias_direction (ndarray): numpy array of shape (embedding dimension,). The\n","        embedding vector representing the bias direction\n","        word_to_vector_map (Dict):  A dictionary mapping words to embedding vectors\n","    Returns:\n","        embedding_word_a (ndarray): numpy array of shape (embedding dimension,). The\n","        embedding vector representing the first word\n","        embedding_word_b (ndarray): numpy array of shape (embedding dimension,). The\n","        embedding vector representing the second word\n","    \"\"\"\n","\n","    # Start code here #\n","    # Get the vector representation of word pair by unpacking equality_set  (~ 3 line)\n","    word_a, word_b = None\n","    embedding_word_a = None\n","    embedding_word_b = None\n","\n","    # Compute the mean (eq. 5) of embedding_word_a and embedding_word_a (~ 1 line)\n","    mean = None\n","\n","    # Compute the projection of mean representation onto the bias direction (eq. 6) (~ 1 line)\n","    mean_B = None\n","\n","    # Compute the projection onto the orthogonal subspace (eq. 7) (~ 1 line)\n","    mean_othorgonal = None\n","\n","    # Compute the projection of th embedding of word a onto the bias direction (eq. 8) (~ 1 line)\n","    embedding_word_a_on_bias_direction = None\n","\n","    # Compute the projection of th embedding of word b onto the bias direction (eq. 9) (~ 1 line)\n","    embedding_word_b_on_bias_direction = None\n","    # Re-embed embedding of word a using eq. 10 (~ 1 long line)\n","    new_embedding_word_a_on_bias_direction = None\n","\n","    # Re-embed embedding of word b using eq. 11 (~ 1 long line)\n","    new_embedding_word_b_on_bias_direction = None\n","\n","    # Equalize embedding of word a using eq. 12 (~ 1 line)\n","    embedding_word_a =  None\n","\n","    # Equalize embedding of word b using eq. 13 (~ 1 line)\n","    embedding_word_b = None\n","\n","    # End code here #\n","\n","    return embedding_word_a, embedding_word_b"],"metadata":{"id":"vMO9XKDf0nO5","executionInfo":{"status":"ok","timestamp":1731779754069,"user_tz":360,"elapsed":14,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}}},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 5b:</font>** Test your implementation by running the cell below.\n","***"],"metadata":{"id":"9lrYzSdlA_Ya"}},{"cell_type":"code","source":["def equalization(equality_set, bias_direction, word_to_vector_map):\n","    \"\"\"\n","    Equalizes the pair of gender-specific words in the equality_set ensuring that\n","    any neutral word is equidistant to all words in the equality_set.\n","\n","    Arguments:\n","        equality_set (Tuple(String, String)): a tuple of strings of gender-specific\n","            words to debias e.g (\"grandmother\", \"grandfather\")\n","        bias_direction (ndarray): numpy array of shape (embedding_dimension,). The\n","            embedding vector representing the bias direction\n","        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n","\n","    Returns:\n","        embedding_word_a (ndarray): numpy array of shape (embedding_dimension,). The\n","            embedding vector representing the first word\n","        embedding_word_b (ndarray): numpy array of shape (embedding_dimension,). The\n","            embedding vector representing the second word\n","    \"\"\"\n","\n","    # Get the vector representation of word pair by unpacking equality_set\n","    word_a, word_b = equality_set\n","    embedding_word_a = word_to_vector_map[word_a]\n","    embedding_word_b = word_to_vector_map[word_b]\n","\n","    # Add this line to return the embedding vectors\n","    return embedding_word_a, embedding_word_b\n","\n","\n","print(\"Cosine similarity before equalization:\")\n","print(f\"(embedding vector of father, gender_direction): {cosine_similarity(word_to_vector_map['father'], gender_direction)}\")\n","print(f\"(embedding vector of mother, gender_direction): {cosine_similarity(word_to_vector_map['mother'], gender_direction)}\")\n","print()\n","\n","embedding_word_a, embedding_word_b  = equalization((\"father\", \"mother\"), gender_direction, word_to_vector_map)\n","print(\"Cosine similarity after equalization:\")\n","print(f\"(embedding vector of father, gender_direction): {cosine_similarity(embedding_word_a, gender_direction)}\")\n","print(f\"(embedding vector of mother, gender_direction): {cosine_similarity(embedding_word_b, gender_direction)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYgSoqLr37Vq","executionInfo":{"status":"ok","timestamp":1731779754069,"user_tz":360,"elapsed":13,"user":{"displayName":"Edward Cruz","userId":"17952678720660941529"}},"outputId":"136eb423-c1fb-4dff-fc0b-8ee9eb4e448d"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity before equalization:\n","(embedding vector of father, gender_direction): -0.08502503175882657\n","(embedding vector of mother, gender_direction): 0.3332593015356538\n","\n","Cosine similarity after equalization:\n","(embedding vector of father, gender_direction): -0.08502503175882657\n","(embedding vector of mother, gender_direction): 0.3332593015356538\n"]}]},{"cell_type":"markdown","source":["***\n","**<font color='red'>Task 5c:</font>** Looking at the output of your implementation test above, what can you observe?.\n","***"],"metadata":{"id":"KJUPPz80BJCW"}},{"cell_type":"markdown","source":["**References**:\n"," - The debiasing algorithm is from Bolukbasi et al., 2016 [Man is to Computer Programmer as Woman is to Homemake? Debiasing word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n"," - The code is partly adapted from Andrew Ng's debiasing word embeddings course on [Coursera](https://www.coursera.org/learn/nlp-sequence-models/lecture/zHASj/debiasing-word-embeddings)\n"," - The GloVe word embeddings is publicly available at (https://nlp.stanford.edu/projects/glove/) and is due to the works of Jeffrey Pennington, Richard Socher, and Christopher D. Manning."],"metadata":{"id":"07sW3hQeZY36"}}]}