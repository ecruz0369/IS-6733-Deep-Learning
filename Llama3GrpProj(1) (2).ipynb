{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a151ef8-1a04-4368-8d38-70dfa47ca582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install accelerate\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31736a58-68fa-4fcc-9100-3c2bb9e9d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch accelerate datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92011a60-3a49-4db6-8baf-a846e1b5e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56eb003238a74840a84f41aa3dcca940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbec52665b2469eb2382fb910594a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "class NumericHeadlineAnalyzer:\n",
    "    def __init__(self, token: str):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with HuggingFace token and Llama model\n",
    "        \n",
    "        Args:\n",
    "            token: HuggingFace API token\n",
    "        \"\"\"\n",
    "        self.model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        # Set the token for model access\n",
    "        self.token = token\n",
    "        os.environ[\"HUGGINGFACE_TOKEN\"] = token\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer parallelism\n",
    "\n",
    "        # Initialize with token authentication\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            token=self.token  # Updated from use_auth_token to token\n",
    "        )\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            token=self.token,  # Updated from use_auth_token to token\n",
    "            device_map=\"auto\",  # Automatically handle GPU if available\n",
    "            torch_dtype=torch.float16  # Use half precision to save memory\n",
    "        )\n",
    "\n",
    "        # Create generation pipeline\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "\n",
    "    def extract_numbers(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract all numbers and numeric expressions from text\"\"\"\n",
    "        number_pattern = r'\\d+\\.?\\d*%?|\\$\\d+\\.?\\d*|\\d+(?:st|nd|rd|th)'\n",
    "        return re.findall(number_pattern, text)\n",
    "\n",
    "    def generate_numeric_headline(self, article_text: str) -> str:\n",
    "        \"\"\"Generate a headline with emphasis on including numeric content\"\"\"\n",
    "        # Create a prompt that emphasizes including numbers in the headline\n",
    "        numbers = self.extract_numbers(article_text)\n",
    "        prompt = f\"\"\"Generate a concise news headline from this article, including relevant numbers if present:\n",
    "\n",
    "Article: {article_text}\n",
    "\n",
    "Headline:\"\"\"\n",
    "        # Generate headline using the model\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=50,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )[0]['generated_text']\n",
    "\n",
    "        # Extract the headline from the response (everything after \"Headline:\")\n",
    "        headline = response.split(\"Headline:\")[-1].strip()\n",
    "\n",
    "        return headline\n",
    "\n",
    "    def analyze_article(self, article: str) -> Dict:\n",
    "        \"\"\"Analyze a single article\"\"\"\n",
    "        headline = self.generate_numeric_headline(article)\n",
    "        numbers = self.extract_numbers(article)\n",
    "\n",
    "        return {\n",
    "            \"generated_headline\": headline,\n",
    "            \"contains_numbers\": bool(numbers),\n",
    "            \"extracted_numbers\": numbers,\n",
    "            \"original_text\": article\n",
    "        }\n",
    "\n",
    "\n",
    "# Your HuggingFace token\n",
    "HUGGINGFACE_TOKEN = \"#TOKEN\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Create analyzer with token\n",
    "        analyzer = NumericHeadlineAnalyzer(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(\"NumHG-main/Dataset/fold-1/test.csv\")\n",
    "        test_articles = data[\"text\"]\n",
    "\n",
    "        # Create a Dataset object\n",
    "        dataset = Dataset.from_pandas(data)\n",
    "\n",
    "        # Define a processing function for batch processing\n",
    "        def process_articles(batch):\n",
    "            articles = batch[\"text\"]\n",
    "            headlines = []\n",
    "            contains_numbers = []\n",
    "            extracted_numbers = []\n",
    "            for article in articles:\n",
    "                headline = analyzer.generate_numeric_headline(article)\n",
    "                numbers = analyzer.extract_numbers(article)\n",
    "                headlines.append(headline)\n",
    "                contains_numbers.append(bool(numbers))\n",
    "                extracted_numbers.append(numbers)\n",
    "\n",
    "            return {\n",
    "                \"generated_headline\": headlines,\n",
    "                \"contains_numbers\": contains_numbers,\n",
    "                \"extracted_numbers\": extracted_numbers\n",
    "            }\n",
    "\n",
    "        # Process the dataset in batches\n",
    "        processed_dataset = dataset.map(process_articles, batched=True, batch_size=8)\n",
    "\n",
    "        # Convert the processed dataset to a DataFrame\n",
    "        processed_df = processed_dataset.to_pandas()\n",
    "\n",
    "        # Save the results to a text file\n",
    "        processed_df[[\"generated_headline\", \"contains_numbers\", \"extracted_numbers\"]].to_csv(\n",
    "            \"results.csv\", index=False\n",
    "        )\n",
    "\n",
    "        print(\"Headline generation complete. Results saved to 'results.csv'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Verify your HuggingFace token has access to meta-llama models.\")\n",
    "        print(\"2. Check your internet connection.\")\n",
    "        print(\"3. Ensure you have enough GPU memory (or try running on CPU).\")\n",
    "        print(\"4. Make sure all required packages are installed:\")\n",
    "        print(\"   pip install transformers torch accelerate datasets\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3122c5-15ea-463a-aedf-6ba0cfe1255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AIPHES/emnlp19-moverscore.git\n",
    "cd emnlp19-moverscore/\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e10a7-8f01-44cb-954c-856795f33b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "python numhg_eval.py \\--tgt_path=target_path \\--pre_path=predict_result_path \\--num_gt_path=numeral_ground_truth_path \\--num_type_path=numeral_type_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc4a79-4533-440c-b06f-3231c398b45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
