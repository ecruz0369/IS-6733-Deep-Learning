{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7lACZZxKsY2"
   },
   "source": [
    "# Debiasing word embeddings\n",
    "\n",
    "Word embedding are word vectors that have meaning, word vectors similar to each other will be close to each other in a vector space.\n",
    "\n",
    "**After completing this lab you will be to:**\n",
    "\n",
    "- Use and load pre-trained word vectors\n",
    "- Measure similarity of word vectors using cosine similarity\n",
    "- Solve word analogy probelms such as Man is to Woman as Boy is to ____ using word embeddings\n",
    "- Reduce gender bias in word embeddings by modifying word embeddings to remove gender stereotypes, such as the association between the words *receptionist* and *female*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTlroLtZPGwh"
   },
   "source": [
    "## <font color='darkblue'>Word embeddings</font>\n",
    "\n",
    "Word embedding is a method used to represent words as vectors. They are populary used in machine learning and natural language processing tasks. Despite their success in downstream tasks such as cyberbullying, sentiment analysis, and question retrieval, they exhibit gender sterotypes which raises concerns because their widespread use can amplify these biases.\n",
    "\n",
    "\n",
    "Word embeddings are trained on word co-occurance using a text dataset. After training, each word $w$ will be represented as a $d$-dimensional word vector $\\vec{w} \\space ϵ \\space ℝ^d $.\n",
    "\n",
    "#### Word embedding properties:\n",
    "* Words with similar semantic meaning will be close to each other\n",
    "* The difference between word embedding vectors can represent relationships between words. For example, given the analogy \"man is to King as woman is to $x$\" (denoted as $man:king :: woman:x$), by doing simple arithmetic on the embedding vectors, we find that $x = queen$ is the best answer because $\\vec{man} - \\vec{woman} ≈ \\vec{king} - \\vec{queen}$. For the analogy $Paris:France :: Nairobi:x$, finds that $x = Kenya$. These embeddings can also amplify sexism implicit in text. For instance, $\\vec{man} - \\vec{woman} ≈ \\vec{computer \\space programmer} - \\vec{homemaker}$. The same system that produced reasonable answers to the previous examples offensively answers \"man is to computer programmer as woman is to $x$\" with $x = homemaker$.\n",
    "\n",
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "LIgOC7jfKe6w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jgtMxNajovM"
   },
   "source": [
    "## <font color='darkblue'>Download and Load word vectors</font>\n",
    "Due to the computational resources required to train word embeddings, we will be using a pre-trained 50-dimensional word embeddings, GloVe to represent words.\n",
    "\n",
    "Run the following cells to download and load the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "rP6R3VgXmMwQ"
   },
   "outputs": [],
   "source": [
    "def download_glove_vectors():\n",
    "    '''\n",
    "    Download the GloVe vectors\n",
    "    Arguments:\n",
    "        None\n",
    "    Returns:\n",
    "        file_name (String): The absolute path of the downloaded 50-dimensional\n",
    "        GloVe word vector representations\n",
    "    '''\n",
    "\n",
    "    if not Path('data').is_dir():\n",
    "        print(\"Downloading the embeddings ...\")\n",
    "        !wget --quiet https://nlp.stanford.edu/data/glove.6B.zip\n",
    "        print(\"Embeddings downloaded.\")\n",
    "\n",
    "        # Unzip it\n",
    "        print(\"Unzipping the downloaded file ...\")\n",
    "        !unzip -q glove.6B.zip -d data/\n",
    "        print(\"File unzipped.\")\n",
    "\n",
    "    return '/content/data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "NdjBx4rsjgTp"
   },
   "outputs": [],
   "source": [
    "def get_glove_vectors(glove_file):\n",
    "    '''\n",
    "    Read the word vectors in glove_file\n",
    "    Arguments:\n",
    "        glove_file (String): The absolute path to the downloaded glove word embeddings\n",
    "    Returns:\n",
    "        words (Set): The words (vocabulary) in the pretrained glove word embeddings\n",
    "        word_to_vector_map (Dict): A dictionary mapping the each word to its embedding vector\n",
    "    '''\n",
    "\n",
    "    words = set()\n",
    "    word_to_vector_map = {}\n",
    "    with open(glove_file, 'r') as file_handle:\n",
    "        for line in file_handle:\n",
    "            line = line.strip().split()\n",
    "            current_word = line[0]\n",
    "            words.add(current_word)\n",
    "            current_word_vector = line[1:]\n",
    "            word_to_vector_map[current_word] = np.array(current_word_vector, dtype=np.float64)\n",
    "\n",
    "    return words, word_to_vector_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kml614/IS6734Labs'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words loaded: 400000\n"
     ]
    }
   ],
   "source": [
    "def get_glove_vectors(zip_file_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe word embeddings from a zip file.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path: Path to the zip file containing the GloVe embeddings.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of:\n",
    "            - A set of words in the vocabulary.\n",
    "            - A dictionary mapping words to their GloVe vector representations.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    word_to_vector_map = {}\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Try to read the file inside the zip archive\n",
    "        with zip_ref.open('glove.6B.50d.txt', 'r') as file_handle:\n",
    "            for line in file_handle:\n",
    "                # Decode the binary content to text\n",
    "                decoded_line = line.decode('utf-8').strip()\n",
    "                split_line = decoded_line.split()\n",
    "                current_word = split_line[0]\n",
    "                vector = np.asarray([float(v) for v in split_line[1:]], dtype='float32')\n",
    "                words.add(current_word)\n",
    "                word_to_vector_map[current_word] = vector\n",
    "\n",
    "    return words, word_to_vector_map\n",
    "\n",
    "# Assuming the zip file is in the specified directory\n",
    "zip_file_path = \"/home/kml614/IS6734Labs/glove.6B.zip\"\n",
    "\n",
    "# Call the function\n",
    "words, word_to_vector_map = get_glove_vectors(zip_file_path)\n",
    "\n",
    "# Use the loaded words and word vectors\n",
    "print(f\"Number of words loaded: {len(words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFF8yxWxq06z"
   },
   "source": [
    "## <font color='darkblue'>Operations on word embeddings</font>\n",
    "\n",
    "### Task 1 - Cosine similarity\n",
    "\n",
    "Similarity between two words represented as word vectors $u$ and $v$ can be measured by their cosine similarity:\n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$u \\cdot v$ is the dot (inner) product of the two vectors\n",
    "\n",
    "$||u||_2$ is the length of the vector $u$. The length also called Euclidean length or Euclidean norm defines a distance function defined as $||u||_2 = \\sqrt{u_1^2 \\space + \\space ... \\space + \\space u_n^2}$\n",
    "\n",
    "The normalized similarity between $u$ and $v$ is the cosine of the angle between the two vectors denoted as $\\theta$. The cosine similarity of $u$ and $v$ will be close to 1 if the two vectors are similar, otherwise, the cosine similarity will be small.\n",
    "\n",
    "**Note**: We will be refering to the embedding of a word i.e the word vector and the word interchangeably in this lab.\n",
    "\n",
    "\n",
    "***\n",
    "**<font color='red'>Task 1a:</font>** Implement equation 1 in the `cosine_similarity()` function below. <br> Hint: check out the numpy documentation on [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html), and [np.sqrt](https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html). Depending on how you choose to implement it, you can check out [np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1a: Implementing Cosine Similarity\n",
    "#The cosine_similarity function measures how closely two vectors align in space. It does this by comparing their directions, \n",
    "#regardless of magnitude, using the formula: dot product of the vectors divided by the product of their magnitudes. \n",
    "#This produces a value between -1 and 1, where 1 indicates identical direction and -1 indicates opposite directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "QJWkmQXh-iK1"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity of two word vectors - vector1 and vector2\n",
    "    Arguments:\n",
    "        vector1 (ndarray): A word vector having shape (n,)\n",
    "        vector2 (ndarray): A word vector having shape (n,)\n",
    "    Returns:\n",
    "        cosine_similarity (float): The cosine similarity between vector1 and vector2\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the dot product between vector1 and vector2\n",
    "    dot = np.dot(vector1, vector2)\n",
    "\n",
    "    # Compute the Euclidean norm or length of vector1\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "\n",
    "    # Compute the Euclidean norm or length of vector2\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "\n",
    "    # Compute the cosine similarity as defined in equation 1\n",
    "    cosine_similarity = dot / (norm_vector1 * norm_vector2)\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "eP-Kxp7YBcDw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between man and woman: 0.8860337734222412\n",
      "Cosine similarity between cat and dog: 0.9218005537986755\n",
      "Cosine similarity between cat and cow: 0.40695685148239136\n",
      "Cosine similarity between england - london and edinburgh - scotland: -0.5203389525413513\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to obtain and report your answers\n",
    "man = word_to_vector_map[\"man\"]\n",
    "woman = word_to_vector_map[\"woman\"]\n",
    "cat = word_to_vector_map[\"cat\"]\n",
    "dog = word_to_vector_map[\"dog\"]\n",
    "orange = word_to_vector_map[\"orange\"]\n",
    "england = word_to_vector_map[\"england\"]\n",
    "london = word_to_vector_map[\"london\"]\n",
    "edinburgh = word_to_vector_map[\"edinburgh\"]\n",
    "scotland = word_to_vector_map[\"scotland\"]\n",
    "\n",
    "print(f\"Cosine similarity between man and woman: {cosine_similarity(man, woman)}\")\n",
    "print(f\"Cosine similarity between cat and dog: {cosine_similarity(cat, dog)}\")\n",
    "print(f\"Cosine similarity between cat and cow: {cosine_similarity(cat, orange)}\")\n",
    "print(f\"Cosine similarity between england - london and edinburgh - scotland: {cosine_similarity(england - london, edinburgh - scotland)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FMC1hZ4njIH"
   },
   "source": [
    "**<font color='red'>Task 1b:</font>** In the code cell below, try out 3 of your own inputs here and report your inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#France and Paris show the highest similarity (0.80), which makes sense since there's a strong geographic relationship - Paris \n",
    "#is France's capital city. King and Queen show similarly high similarity (0.78), reflecting their closely related royal roles \n",
    "#and positions.\n",
    "#Apple and Fruit show moderate similarity (0.59), which is expected since apple is a type of fruit - there's a clear \n",
    "#relationship, but it's not as strong as the other pairs since 'fruit'is a broader category term while 'apple' is a \n",
    "#specific instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between king and queen: 0.7839043140411377\n",
      "Cosine similarity between france and paris: 0.8025329113006592\n",
      "Cosine similarity between apple and fruit: 0.5917636156082153\n"
     ]
    }
   ],
   "source": [
    "# Start code here #\n",
    "king = word_to_vector_map[\"king\"]\n",
    "queen = word_to_vector_map[\"queen\"]\n",
    "france = word_to_vector_map[\"france\"]\n",
    "paris = word_to_vector_map[\"paris\"]\n",
    "apple = word_to_vector_map[\"apple\"]\n",
    "fruit = word_to_vector_map[\"fruit\"]\n",
    "\n",
    "print(f\"Cosine similarity between king and queen: {cosine_similarity(king, queen)}\")\n",
    "print(f\"Cosine similarity between france and paris: {cosine_similarity(france, paris)}\")\n",
    "print(f\"Cosine similarity between apple and fruit: {cosine_similarity(apple, fruit)}\")\n",
    "# End code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmsetI1rFUcy"
   },
   "source": [
    "### Task 2 - Word analogy\n",
    "\n",
    "In an analogy task, you are given an analogy in the form \"i is to j as k is to ___\". Your task is to complete this sentence.\n",
    "\n",
    "For example, if you are given \"man is to king as woman is to $l$\" (denoted as $man:king :: woman:l$). You are to find the best word $l$ that answers the analogy the best. Simple arithmetic of the embedding vectors will find that $l = queen$ is the best answer because the embedding vectors of words $i$, $j$, $k$, and $l$ denoted as $e_i$, $e_j$, $e_k$, $e_l$ have the following relationship:\n",
    "$$e_j - e_i ≈ e_l - e_k$$\n",
    "\n",
    "Cosine similarity can be used to measure the similarity between $e_j - e_i$ and $e_l - e_k$\n",
    "\n",
    "***\n",
    "**<font color='red'>Task 2a:</font>** To perform word analogies, implement `answer_analogy()` below.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embeddings can solve analogies by using vector arithmetic - if we subtract and add the right vectors, \n",
    "#we can find relationships ike \"France is to Paris as China is to Beijing.\" The model finds the word whose \n",
    "#vector is most similar to the result of this \n",
    "#arithmetic (Paris - France + China ≈ Beijing), showing how embeddings capture meaningful semantic \n",
    "#relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed the issue this code now works Nov 14, 2024 ECruz\n",
    "def answer_analogy(word_i, word_j, word_k, word_to_vector_map):\n",
    "    \"\"\"\n",
    "    Performs word analogy as described above.\n",
    "    Arguments:\n",
    "        word_i (str): A word.\n",
    "        word_j (str): A word.\n",
    "        word_k (str): A word.\n",
    "        word_to_vector_map (dict): A dictionary mapping words to embedding vectors.\n",
    "    Returns:\n",
    "        best_word (str): A word that fulfills the analogy, based on cosine similarity.\n",
    "    \"\"\"\n",
    "    # Convert words to lowercase\n",
    "    word_i, word_j, word_k = word_i.lower(), word_j.lower(), word_k.lower()\n",
    "\n",
    "    # Get embedding vectors for the input words\n",
    "    try:\n",
    "        embedding_vector_of_word_i = word_to_vector_map[word_i]\n",
    "        embedding_vector_of_word_j = word_to_vector_map[word_j]\n",
    "        embedding_vector_of_word_k = word_to_vector_map[word_k]\n",
    "    except KeyError as e:\n",
    "        print(f\"{e.args[0]} is not in the vocabulary. Please try a different word.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize variables for tracking the best word\n",
    "    words = word_to_vector_map.keys()\n",
    "    max_cosine_similarity = -1e9\n",
    "    best_word = None\n",
    "    input_words = {word_i, word_j, word_k}\n",
    "\n",
    "    for word in words:\n",
    "        if word in input_words:\n",
    "            continue\n",
    "\n",
    "        # Compute the cosine similarity\n",
    "        embedding_vector_of_word_l = word_to_vector_map[word]\n",
    "        a = embedding_vector_of_word_j - embedding_vector_of_word_i\n",
    "        b = embedding_vector_of_word_l - embedding_vector_of_word_k\n",
    "        similarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        # Update the best word if similarity is higher\n",
    "        if similarity > max_cosine_similarity:\n",
    "            max_cosine_similarity = similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzFBmLKKyDD8"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 2b:</font>** Test your implementation by running the code cell below. What are your observations? What do you observe about the last two outputs?.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common linguistic patterns like \"france:french::germany:german\" are handled well, showing the model's strength with \n",
    "#regular language relationships.\n",
    "#However, when testing \"man:doctor::woman:nurse,\" the model reveals concerning gender biases present in its training data, \n",
    "#defaulting to stereotypical professional associations rather than maintaining equivalent roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "wSB0BR5YvPg0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france -> french :: germany -> german\n",
      "england -> london :: japan -> tokyo\n",
      "boy -> girl :: man -> woman\n",
      "man -> doctor :: woman -> nurse\n",
      "small -> smaller :: big -> competitors\n"
     ]
    }
   ],
   "source": [
    "analogies = [('france', 'french', 'germany'),\n",
    "             ('england', 'london', 'japan'),\n",
    "             ('boy', 'girl', 'man'),\n",
    "             ('man', 'doctor', 'woman'),\n",
    "             ('small', 'smaller', 'big')]\n",
    "for analogy in analogies:\n",
    "    best_word = answer_analogy(*analogy, word_to_vector_map)\n",
    "    if best_word:\n",
    "        print(f\"{analogy[0]} -> {analogy[1]} :: {analogy[2]} -> {best_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFAAQaTSw1cJ"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 2c:</font>** Try your own analogies by completing and executing the code cell below. Find 2 that works and one that doesn't. Report your inputs and outputs\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king -> queen :: prince -> princess\n",
      "paris -> france :: nice -> croatia\n",
      "car -> road :: sign -> transfrontier\n",
      "brother -> sister :: uncle -> aunt\n",
      "teacher -> school :: doctor -> college\n",
      "cat -> kitten :: dog -> warmonger\n"
     ]
    }
   ],
   "source": [
    "# Define your own analogies here\n",
    "# Define your own analogies here\n",
    "my_analogies = [\n",
    "    ('king', 'queen', 'prince'),          \n",
    "    ('paris', 'france', 'nice'),          \n",
    "    ('car', 'road', 'sign'),              \n",
    "    ('brother', 'sister', 'uncle'),      \n",
    "    ('teacher', 'school', 'doctor'),      \n",
    "    ('cat', 'kitten', 'dog'),             \n",
    "]   \n",
    "\n",
    "# Execute the analogies\n",
    "for analogy in my_analogies:\n",
    "    best_word = answer_analogy(*analogy, word_to_vector_map)\n",
    "    print(f\"{analogy[0]} -> {analogy[1]} :: {analogy[2]} -> {best_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sbFypomLRbE"
   },
   "source": [
    "### Task 3 - Geometry of Gender and Bias in Word Embeddings: Occupational stereotypes\n",
    "In this task, we will understand the biases present in word-embedding i.e which words are closer to $she$ than to $he$. This will be achieved by evaluating whether the GloVe embeddings have sterotypes on occupation words. Determine gender bias by projecting each of the occupations onto the $she - he$ direction by computing the dot product between each occupation word embedding and the embedding vector of $she - he$ normalized by the Euclidean norm (See task 1).\n",
    "\n",
    "$$occupation\\_word_i \\cdot ||she - he||_2 \\tag{2}$$\n",
    "\n",
    "Notice that equation 2 is similar to only the numerator of equation 1 because we are computing the dot product of $occupation\\_word_i$ and the normalized difference between $she$ and $he$.\n",
    "\n",
    "Run the cells below to download and view the occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Rg5hB08IX8PB"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def download_occupations():\n",
    "    if not Path('debiaswe').is_dir():\n",
    "        print(\"Downloading occupation list ...\")\n",
    "        !git clone -q https://github.com/tolga-b/debiaswe.git\n",
    "        print(\"Occupation list downloaded.\")\n",
    "    else:\n",
    "        print(\"Repository already exists.\")\n",
    "    return 'debiaswe/data/professions.json'\n",
    "\n",
    "def view_occupations(occupations_file):\n",
    "    if not Path(occupations_file).is_file():\n",
    "        print(f\"File not found: {occupations_file}\")\n",
    "        return\n",
    "\n",
    "    with open(occupations_file, 'r') as file_handle:\n",
    "        occupations = json.load(file_handle)\n",
    "\n",
    "        for occupation in occupations:\n",
    "            print(occupation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "gpWAw2r4oK59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists.\n"
     ]
    }
   ],
   "source": [
    "occupations_file = download_occupations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "O1dlznxmliBJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountant\n",
      "acquaintance\n",
      "actor\n",
      "actress\n",
      "adjunct_professor\n",
      "administrator\n",
      "adventurer\n",
      "advocate\n",
      "aide\n",
      "alderman\n",
      "alter_ego\n",
      "ambassador\n",
      "analyst\n",
      "anthropologist\n",
      "archaeologist\n",
      "archbishop\n",
      "architect\n",
      "artist\n",
      "artiste\n",
      "assassin\n",
      "assistant_professor\n",
      "associate_dean\n",
      "associate_professor\n",
      "astronaut\n",
      "astronomer\n",
      "athlete\n",
      "athletic_director\n",
      "attorney\n",
      "author\n",
      "baker\n",
      "ballerina\n",
      "ballplayer\n",
      "banker\n",
      "barber\n",
      "baron\n",
      "barrister\n",
      "bartender\n",
      "biologist\n",
      "bishop\n",
      "bodyguard\n",
      "bookkeeper\n",
      "boss\n",
      "boxer\n",
      "broadcaster\n",
      "broker\n",
      "bureaucrat\n",
      "businessman\n",
      "businesswoman\n",
      "butcher\n",
      "butler\n",
      "cab_driver\n",
      "cabbie\n",
      "cameraman\n",
      "campaigner\n",
      "captain\n",
      "cardiologist\n",
      "caretaker\n",
      "carpenter\n",
      "cartoonist\n",
      "cellist\n",
      "chancellor\n",
      "chaplain\n",
      "character\n",
      "chef\n",
      "chemist\n",
      "choreographer\n",
      "cinematographer\n",
      "citizen\n",
      "civil_servant\n",
      "cleric\n",
      "clerk\n",
      "coach\n",
      "collector\n",
      "colonel\n",
      "columnist\n",
      "comedian\n",
      "comic\n",
      "commander\n",
      "commentator\n",
      "commissioner\n",
      "composer\n",
      "conductor\n",
      "confesses\n",
      "congressman\n",
      "constable\n",
      "consultant\n",
      "cop\n",
      "correspondent\n",
      "councilman\n",
      "councilor\n",
      "counselor\n",
      "critic\n",
      "crooner\n",
      "crusader\n",
      "curator\n",
      "custodian\n",
      "dad\n",
      "dancer\n",
      "dean\n",
      "dentist\n",
      "deputy\n",
      "dermatologist\n",
      "detective\n",
      "diplomat\n",
      "director\n",
      "disc_jockey\n",
      "doctor\n",
      "doctoral_student\n",
      "drug_addict\n",
      "drummer\n",
      "economics_professor\n",
      "economist\n",
      "editor\n",
      "educator\n",
      "electrician\n",
      "employee\n",
      "entertainer\n",
      "entrepreneur\n",
      "environmentalist\n",
      "envoy\n",
      "epidemiologist\n",
      "evangelist\n",
      "farmer\n",
      "fashion_designer\n",
      "fighter_pilot\n",
      "filmmaker\n",
      "financier\n",
      "firebrand\n",
      "firefighter\n",
      "fireman\n",
      "fisherman\n",
      "footballer\n",
      "foreman\n",
      "freelance_writer\n",
      "gangster\n",
      "gardener\n",
      "geologist\n",
      "goalkeeper\n",
      "graphic_designer\n",
      "guidance_counselor\n",
      "guitarist\n",
      "hairdresser\n",
      "handyman\n",
      "headmaster\n",
      "historian\n",
      "hitman\n",
      "homemaker\n",
      "hooker\n",
      "housekeeper\n",
      "housewife\n",
      "illustrator\n",
      "industrialist\n",
      "infielder\n",
      "inspector\n",
      "instructor\n",
      "interior_designer\n",
      "inventor\n",
      "investigator\n",
      "investment_banker\n",
      "janitor\n",
      "jeweler\n",
      "journalist\n",
      "judge\n",
      "jurist\n",
      "laborer\n",
      "landlord\n",
      "lawmaker\n",
      "lawyer\n",
      "lecturer\n",
      "legislator\n",
      "librarian\n",
      "lieutenant\n",
      "lifeguard\n",
      "lyricist\n",
      "maestro\n",
      "magician\n",
      "magistrate\n",
      "maid\n",
      "major_leaguer\n",
      "manager\n",
      "marksman\n",
      "marshal\n",
      "mathematician\n",
      "mechanic\n",
      "mediator\n",
      "medic\n",
      "midfielder\n",
      "minister\n",
      "missionary\n",
      "mobster\n",
      "monk\n",
      "musician\n",
      "nanny\n",
      "narrator\n",
      "naturalist\n",
      "negotiator\n",
      "neurologist\n",
      "neurosurgeon\n",
      "novelist\n",
      "nun\n",
      "nurse\n",
      "observer\n",
      "officer\n",
      "organist\n",
      "painter\n",
      "paralegal\n",
      "parishioner\n",
      "parliamentarian\n",
      "pastor\n",
      "pathologist\n",
      "patrolman\n",
      "pediatrician\n",
      "performer\n",
      "pharmacist\n",
      "philanthropist\n",
      "philosopher\n",
      "photographer\n",
      "photojournalist\n",
      "physician\n",
      "physicist\n",
      "pianist\n",
      "planner\n",
      "plastic_surgeon\n",
      "playwright\n",
      "plumber\n",
      "poet\n",
      "policeman\n",
      "politician\n",
      "pollster\n",
      "preacher\n",
      "president\n",
      "priest\n",
      "principal\n",
      "prisoner\n",
      "professor\n",
      "professor_emeritus\n",
      "programmer\n",
      "promoter\n",
      "proprietor\n",
      "prosecutor\n",
      "protagonist\n",
      "protege\n",
      "protester\n",
      "provost\n",
      "psychiatrist\n",
      "psychologist\n",
      "publicist\n",
      "pundit\n",
      "rabbi\n",
      "radiologist\n",
      "ranger\n",
      "realtor\n",
      "receptionist\n",
      "registered_nurse\n",
      "researcher\n",
      "restaurateur\n",
      "sailor\n",
      "saint\n",
      "salesman\n",
      "saxophonist\n",
      "scholar\n",
      "scientist\n",
      "screenwriter\n",
      "sculptor\n",
      "secretary\n",
      "senator\n",
      "sergeant\n",
      "servant\n",
      "serviceman\n",
      "sheriff_deputy\n",
      "shopkeeper\n",
      "singer\n",
      "singer_songwriter\n",
      "skipper\n",
      "socialite\n",
      "sociologist\n",
      "soft_spoken\n",
      "soldier\n",
      "solicitor\n",
      "solicitor_general\n",
      "soloist\n",
      "sportsman\n",
      "sportswriter\n",
      "statesman\n",
      "steward\n",
      "stockbroker\n",
      "strategist\n",
      "student\n",
      "stylist\n",
      "substitute\n",
      "superintendent\n",
      "surgeon\n",
      "surveyor\n",
      "swimmer\n",
      "taxi_driver\n",
      "teacher\n",
      "technician\n",
      "teenager\n",
      "therapist\n",
      "trader\n",
      "treasurer\n",
      "trooper\n",
      "trucker\n",
      "trumpeter\n",
      "tutor\n",
      "tycoon\n",
      "undersecretary\n",
      "understudy\n",
      "valedictorian\n",
      "vice_chancellor\n",
      "violinist\n",
      "vocalist\n",
      "waiter\n",
      "waitress\n",
      "warden\n",
      "warrior\n",
      "welder\n",
      "worker\n",
      "wrestler\n",
      "writer\n"
     ]
    }
   ],
   "source": [
    "view_occupations(occupations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci7j7szxzF53"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 3a:</font>** Complete the `get_occupation_stereotypes()` below.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occupation_stereotypes(she, he, occupations_file, word_to_vector_map, verbose=False):\n",
    "    \"\"\"\n",
    "    occupation_words = [occupation[0] for occupation in occupations]\n",
    "    Arguments:\n",
    "        she (String): A word\n",
    "        he (String): A word\n",
    "        occupations_file (String): The path to the occupation file\n",
    "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
    "    Returns:\n",
    "        most_similar_words (Tuple(List[Tuple(Float, String)], List[Tuple(Float, String)])):\n",
    "        A tuple of the list of the most similar occupation words to she and he with their associated similarity\n",
    "  \n",
    "    \"\"\"\n",
    "    # Read occupations\n",
    "    with open(occupations_file, 'r') as file_handle:\n",
    "        occupations = json.load(file_handle)\n",
    "    \n",
    "    # Extract occupation words\n",
    "    occupation_words = [occupation[0] for occupation in occupations]\n",
    "    print(f\"Loaded {len(occupation_words)} occupation words.\")\n",
    "\n",
    "    # Get embedding vector of she\n",
    "    embedding_vector_she = word_to_vector_map.get(she)\n",
    "    embedding_vector_he = word_to_vector_map.get(he)\n",
    "    if embedding_vector_she is None or embedding_vector_he is None:\n",
    "        print(\"Error: 'she' or 'he' not found in the word embedding vocabulary.\")\n",
    "        return [], []\n",
    "\n",
    "    # Compute normalized vector difference\n",
    "    vector_difference_she_he = embedding_vector_she - embedding_vector_he\n",
    "    normalized_difference_she_he = vector_difference_she_he / np.linalg.norm(vector_difference_she_he)\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = []\n",
    "    for word in occupation_words:\n",
    "        try:\n",
    "            occupation_word_embedding_vector = word_to_vector_map[word]\n",
    "            similarity = np.dot(occupation_word_embedding_vector, normalized_difference_she_he) / np.linalg.norm(occupation_word_embedding_vector)\n",
    "            similarities.append((similarity, word))\n",
    "        except KeyError:\n",
    "            if verbose:\n",
    "                print(f\"'{word}' not found in vocabulary.\")\n",
    "\n",
    "    # Sort and return top and bottom results\n",
    "    most_similar_words = sorted(similarities)\n",
    "    print(f\"Found {len(most_similar_words)} similarities computed.\")\n",
    "    return most_similar_words[:20], most_similar_words[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 320 occupation words.\n",
      "'adjunct_professor' not found in vocabulary.\n",
      "'alter_ego' not found in vocabulary.\n",
      "'assistant_professor' not found in vocabulary.\n",
      "'associate_dean' not found in vocabulary.\n",
      "'associate_professor' not found in vocabulary.\n",
      "'athletic_director' not found in vocabulary.\n",
      "'cab_driver' not found in vocabulary.\n",
      "'civil_servant' not found in vocabulary.\n",
      "'disc_jockey' not found in vocabulary.\n",
      "'doctoral_student' not found in vocabulary.\n",
      "'drug_addict' not found in vocabulary.\n",
      "'economics_professor' not found in vocabulary.\n",
      "'fashion_designer' not found in vocabulary.\n",
      "'fighter_pilot' not found in vocabulary.\n",
      "'freelance_writer' not found in vocabulary.\n",
      "'graphic_designer' not found in vocabulary.\n",
      "'guidance_counselor' not found in vocabulary.\n",
      "'interior_designer' not found in vocabulary.\n",
      "'investment_banker' not found in vocabulary.\n",
      "'major_leaguer' not found in vocabulary.\n",
      "'plastic_surgeon' not found in vocabulary.\n",
      "'professor_emeritus' not found in vocabulary.\n",
      "'registered_nurse' not found in vocabulary.\n",
      "'sheriff_deputy' not found in vocabulary.\n",
      "'singer_songwriter' not found in vocabulary.\n",
      "'soft_spoken' not found in vocabulary.\n",
      "'solicitor_general' not found in vocabulary.\n",
      "'taxi_driver' not found in vocabulary.\n",
      "'vice_chancellor' not found in vocabulary.\n",
      "Found 291 similarities computed.\n",
      "Top 20 most similar: [(-0.35621235, 'coach'), (-0.33694607, 'caretaker'), (-0.31634083, 'captain'), (-0.30927327, 'marshal'), (-0.30729136, 'colonel'), (-0.30248713, 'skipper'), (-0.30214384, 'manager'), (-0.3016537, 'midfielder'), (-0.29967117, 'archbishop'), (-0.2944306, 'commander'), (-0.29028675, 'footballer'), (-0.2888985, 'bishop'), (-0.28199962, 'marksman'), (-0.27963883, 'firebrand'), (-0.27878764, 'provost'), (-0.2780793, 'substitute'), (-0.27217972, 'lieutenant'), (-0.2719793, 'custodian'), (-0.2719191, 'superintendent'), (-0.27134648, 'goalkeeper')]\n",
      "Bottom 20 least similar: [(0.32072738, 'singer'), (0.3219568, 'publicist'), (0.34405202, 'nanny'), (0.34466952, 'therapist'), (0.34746587, 'confesses'), (0.35577607, 'businesswoman'), (0.35730487, 'dancer'), (0.36456618, 'hairdresser'), (0.3698354, 'receptionist'), (0.37291065, 'housekeeper'), (0.37309748, 'homemaker'), (0.3812397, 'housewife'), (0.38133988, 'nurse'), (0.3892646, 'narrator'), (0.41383365, 'maid'), (0.42853132, 'socialite'), (0.44343776, 'waitress'), (0.44732913, 'stylist'), (0.46861386, 'ballerina'), (0.49840298, 'actress')]\n"
     ]
    }
   ],
   "source": [
    "top_20, bottom_20 = get_occupation_stereotypes('she', 'he', occupations_file, word_to_vector_map, verbose=True)\n",
    "print(\"Top 20 most similar:\", top_20)\n",
    "print(\"Bottom 20 least similar:\", bottom_20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbrKEw6NzMea"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 3b:</font>** Execute the cell below and report your results.\n",
    "\n",
    "1) Does the GloVe word embeddings propagate bias? why?\n",
    "\n",
    "2) From the list associated with she, list those that reflect gender stereotype.   \n",
    "\n",
    "3) Compare your list from 2 to the occupations closest to he. What are your conclusions?\n",
    "\n",
    "Exclude businesswoman from your list.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "66nKdC4Tjwoe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 320 occupation words.\n",
      "Found 291 similarities computed.\n",
      "Occupations closest to he:\n",
      "(-0.35621235, 'coach')\n",
      "(-0.33694607, 'caretaker')\n",
      "(-0.31634083, 'captain')\n",
      "(-0.30927327, 'marshal')\n",
      "(-0.30729136, 'colonel')\n",
      "(-0.30248713, 'skipper')\n",
      "(-0.30214384, 'manager')\n",
      "(-0.3016537, 'midfielder')\n",
      "(-0.29967117, 'archbishop')\n",
      "(-0.2944306, 'commander')\n",
      "(-0.29028675, 'footballer')\n",
      "(-0.2888985, 'bishop')\n",
      "(-0.28199962, 'marksman')\n",
      "(-0.27963883, 'firebrand')\n",
      "(-0.27878764, 'provost')\n",
      "(-0.2780793, 'substitute')\n",
      "(-0.27217972, 'lieutenant')\n",
      "(-0.2719793, 'custodian')\n",
      "(-0.2719191, 'superintendent')\n",
      "(-0.27134648, 'goalkeeper')\n",
      "\n",
      "Occupations closest to she:\n",
      "(0.32072738, 'singer')\n",
      "(0.3219568, 'publicist')\n",
      "(0.34405202, 'nanny')\n",
      "(0.34466952, 'therapist')\n",
      "(0.34746587, 'confesses')\n",
      "(0.35730487, 'dancer')\n",
      "(0.36456618, 'hairdresser')\n",
      "(0.3698354, 'receptionist')\n",
      "(0.37291065, 'housekeeper')\n",
      "(0.37309748, 'homemaker')\n",
      "(0.3812397, 'housewife')\n",
      "(0.38133988, 'nurse')\n",
      "(0.3892646, 'narrator')\n",
      "(0.41383365, 'maid')\n",
      "(0.42853132, 'socialite')\n",
      "(0.44343776, 'waitress')\n",
      "(0.44732913, 'stylist')\n",
      "(0.46861386, 'ballerina')\n",
      "(0.49840298, 'actress')\n"
     ]
    }
   ],
   "source": [
    "he, she = get_occupation_stereotypes('she', 'he', occupations_file, word_to_vector_map)\n",
    "\n",
    "print(\"Occupations closest to he:\")\n",
    "for occupation in he:\n",
    "    print(f\"{occupation[0], occupation[1]}\")\n",
    "\n",
    "print(\"\\nOccupations closest to she:\")\n",
    "for occupation in she:\n",
    "    if occupation[1] != 'businesswoman': # Excluding businesswoman\n",
    "        print(f\"{occupation[0], occupation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear Gender Bias Evidence:\n",
    "#1. Gender Bias Evidence:\n",
    "#The GloVe word embeddings reveal striking gender-based occupational divisions. Terms associated with \"he\" are predominantly \n",
    "#linked to positions of authority and leadership, such as commander, captain, manager, and colonel, along with sports-related \n",
    "#roles like midfielder, footballer, and goalkeeper. In contrast, words closest to \"she\" reflect deeply ingrained gender \n",
    "#stereotypes, clustering around domestic roles (housewife, homemaker, maid), nurturing positions (nurse, nanny), \n",
    "#and appearance-focused occupations (hairdresser, stylist, ballerina).\n",
    "#2. Stereotypical Female Occupations:\n",
    "#The occupations associated with \"she\" reveal particularly concerning stereotypes that reinforce traditional gender roles. \n",
    "#These roles fall into distinct categories: domestic duties (housewife, housekeeper, maid), caregiving positions (nurse, \n",
    "#nanny), \n",
    "#service and appearance-focused jobs (waitress, hairdresser, stylist), and entertainment/aesthetic roles (ballerina, \n",
    "#dancer, actress). \n",
    "#This clustering demonstrates how the embeddings have captured and preserved societal expectations about \"women's work.\"\n",
    "#3. Comparative Analysis:\n",
    "#The stark contrast between male and female occupational associations reveals a troubling power dynamic in the embeddings. \n",
    "#While \"he\" is associated with positions of authority, leadership, and physical activity, \"she\" is linked to supportive, \n",
    "#domestic,and service-oriented roles. This imbalance is particularly evident when comparing high-status positions like \n",
    "#commander/colonel (he) ith service roles like maid/receptionist (she), highlighting how these embeddings reflect and \n",
    "#potentially perpetuate existing ocietal power structures and gender inequalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4dyeTcJxqdA"
   },
   "source": [
    "<!--\n",
    "### Task 4 - Analogies exhibiting stereotypes\n",
    "Using analogies to quantify gender stereotype in the embedding. Given two words, e.g. $he$, $she$, generate a pair of words, $x$ and $y$, such that $he$ to $x$ as $she$ to $y$ is a good analogy. This will generate pairs that the embedding believes to be analogous to $he$, $she$ or any other pair of seed words.\n",
    "\n",
    "The analogy generator takes as input a pair of seed words (a, b) which determines the seed direction $\\vec{a} - \\vec{b}$ corresponding to the ***normalized*** difference between the two seed words.\n",
    "\n",
    "In this task, we will use $(a, b) = (she, he)$. Then all pairs of words $x, y$ is scored using the following metric:\n",
    "\n",
    "$$S_{(a, b)}(x, y) = ||\\vec{a} - \\vec{b}||_2 \\cdot ||\\vec{x} - \\vec{y}||_2 \\space \\text{if} \\space ||\\vec{x} - \\vec{y}||_2 ≤ δ, 0 \\space \\text{else}$$\n",
    "\n",
    "Where $δ$ is a threshold for semantic similarity. We will use $δ = 1$.\n",
    "\n",
    "<!-- In other words, the above equation reads, if the normalized difference between $x$ and $y$ is less than or equal to our threshold, then the score of the pair of words $x, y$ is the dot product of the normalized difference between the seed pairs and the normalized difference between the pair of words $x, y$. ->\n",
    "\n",
    "Notice that each vector difference is normalized, therefore we are basically computing the numerator of equation 1 as part of this equation.\n",
    "\n",
    "***\n",
    "**<font color='red'>Task 4:</font>** Test the implementation of your `get_analogies_exhibiting_stereotypes()` below. Report your results. Are the generated analogies biased?\n",
    "***\n",
    "-->\n",
    "\n",
    "### Task 4 - Debiasing word embeddings\n",
    "\n",
    "**Gender Specific words**\n",
    "\n",
    "Words that are associated with a gender by definition. For example, brother, sister, businesswoman or businessman.\n",
    "\n",
    "**Gender neutral words**\n",
    "\n",
    "The remanining words that are not specific to a gender are gender neutral. For example, flight attendant or shoes. The compliment of gender specific words, can be taken as the gender neutral words.\n",
    "\n",
    "**Step 1 - Identify gender subspace i.e identify the direction of the embedding that captures the bias**\n",
    "\n",
    "To robustly estimate bias, we use the gender specific words to learn a gender subpace in the embedding. To identify the gender subspace, we consider the vector difference of gender specific word pairs, such as $\\vec{she} - \\vec{he}$, $\\vec{woman} - \\vec{man}$ or $\\vec{her} - \\vec{his}$. This identifies a **gender direction or bias subspace** $g \\space ϵ \\space ℝ^d$ which captures gender in the embedding.\n",
    "\n",
    "**Note:** We will use $g$ and $bias\\_direction$ interchangeably in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "1w_fti_WxzCZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.261302    0.438481   -0.13376004  0.12281001  0.00838     0.64455\n",
      "  0.13150996  0.01198     0.73557    -0.04754001 -0.04260999 -0.23386998\n",
      "  0.56951     0.24359     0.29471004  0.152461   -0.44637996  0.08563\n",
      "  0.66735    -0.20257801  0.28133     0.71557     0.04014999  0.42204\n",
      "  0.63574     0.11930013 -0.429694    0.216301    0.08826    -0.5115\n",
      " -0.28599977  0.227249    0.25811     0.18074998 -0.22733    -0.15184401\n",
      " -0.13196    -0.14411998  0.01708999 -0.62810004  0.124465   -0.16902\n",
      "  0.62446666 -0.53734     0.379254   -0.3373      0.38487598 -0.92383\n",
      " -0.019064    0.435641  ]\n"
     ]
    }
   ],
   "source": [
    "gender = word_to_vector_map['she'] - word_to_vector_map['he']\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrsBGsZPcEte"
   },
   "source": [
    "The gender subspace can also be captured more accurately by taking gender pair difference vectors and computing its principal components (PCs). The top PC, denoted by the unit vector $g$, captures the gender subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "C2gVI7TvcvDp"
   },
   "outputs": [],
   "source": [
    "def get_gender_subspace(pairs, word_to_vector_map, num_components=10):\n",
    "    \"\"\"\n",
    "    Compute the gender subspace by computing the principal components of\n",
    "    ten gender pair vectors.\n",
    "    Arguments:\n",
    "        pairs (List[Tuple(String, String)]): A list of gender specific word pairs\n",
    "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
    "        num_components (Int): The number of principal components to compute. Defaults to 10\n",
    "    Returns:\n",
    "        gender_subspace (ndarray): The gender bias subspace(or direction) of shape (embedding dimension,)\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = []\n",
    "    for word_1, word_2 in pairs:\n",
    "        embedding_vector_word_1 = word_to_vector_map[word_1]\n",
    "        embedding_vector_word_2 = word_to_vector_map[word_2]\n",
    "        center = (embedding_vector_word_1 + embedding_vector_word_2) / 2\n",
    "        matrix.append(embedding_vector_word_1 - center)\n",
    "        matrix.append(embedding_vector_word_2 - center)\n",
    "\n",
    "    matrix = np.array(matrix)\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(matrix)\n",
    "\n",
    "    pcs = pca.components_                  # Sorted by decreasing explained variance\n",
    "    eigenvalues = pca.explained_variance_  # Eigenvalues\n",
    "    gender_subspace = pcs[0]               # The first element has the highest eigenvalue\n",
    "    return gender_subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "3qsDDbxTgYSO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06639123  0.15316701 -0.12170386  0.02910501 -0.012115    0.29561922\n",
      "  0.10015247  0.03503808  0.2760534  -0.06259266  0.04843717 -0.20243709\n",
      "  0.22435015  0.02205075  0.08795604  0.05350635 -0.23457442 -0.0051648\n",
      "  0.29097     0.02894429  0.10423078  0.24379618  0.05296572  0.17222571\n",
      "  0.13557158  0.13746522 -0.05081973  0.11252052  0.01639265 -0.21136859\n",
      " -0.14034708  0.13498116  0.08092434  0.02423978 -0.10780552 -0.05927322\n",
      " -0.04857578 -0.03199023  0.08174042 -0.17759706 -0.02782479 -0.16880813\n",
      "  0.27589145 -0.18007477  0.04123207 -0.09385727  0.11011446 -0.2565001\n",
      "  0.06258361  0.00847158]\n"
     ]
    }
   ],
   "source": [
    "gender_specific_pairs = [\n",
    "    ('she', 'he'),\n",
    "    ('her', 'his'),\n",
    "    ('woman', 'man'),\n",
    "    ('mary', 'john'),\n",
    "    ('herself', 'himself'),\n",
    "    ('daughter', 'son'),\n",
    "    ('mother', 'father'),\n",
    "    ('gal', 'guy'),\n",
    "    ('girl', 'boy'),\n",
    "    ('female', 'male')\n",
    "]\n",
    "gender_direction = get_gender_subspace(gender_specific_pairs, word_to_vector_map)\n",
    "print(gender_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dqlFhLsXhsA"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 4a:</font>** Run the cell below to computes the similarity between the gender embedding and the embedding vectors of male and female names. What can you observe?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The analysis of name embeddings reveals clear gender-based patterns across two different subspace methods. In the Simple \n",
    "#Gender Subspace, female names consistently show positive correlation values, with Mary scoring highest at 0.35, \n",
    "#followed by Angela at 0.26, and Sweta at 0.17. Conversely, male names demonstrate negative correlations, \n",
    "#with Kazim showing the strongest negative association at -0.33, followed by John at -0.18,and David at -0.13.\n",
    "#When examining the PCA Based Gender Subspace, the gender associations become even more pronounced, particularly for \n",
    "#male names. \n",
    "#While female names maintain positive correlations (Mary at 0.26, Angela at 0.19, and Sweta at 0.18), \n",
    "#male names show stronger negative associations compared to the simple subspace method, with John at -0.38, \n",
    "#and both David and Kazim around -0.32. This suggests that the PCA-based approach might be more effective at \n",
    "#identifying and quantifying gender-related patterns within the embedding space, particularly for male names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "V0dQ7vU6XRqs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names and their similarities with simple gender subspace\n",
      "mary 0.3457399\n",
      "john -0.17879784\n",
      "sweta 0.17016456\n",
      "david -0.13322614\n",
      "kazim -0.3265896\n",
      "angela 0.26007992\n",
      "\n",
      "Names and their similarities with PCA based gender subspace\n",
      "mary 0.26370916\n",
      "john -0.38168395\n",
      "sweta 0.17737049\n",
      "david -0.3165648\n",
      "kazim -0.32498384\n",
      "angela 0.18623307\n"
     ]
    }
   ],
   "source": [
    "print('Names and their similarities with simple gender subspace')\n",
    "names = [\"mary\", \"john\", \"sweta\", \"david\", \"kazim\", \"angela\"]\n",
    "for name in names:\n",
    "    print(name, cosine_similarity(word_to_vector_map[name], gender))\n",
    "\n",
    "print()\n",
    "print('Names and their similarities with PCA based gender subspace')\n",
    "names = [\"mary\", \"john\", \"sweta\", \"david\", \"kazim\", \"angela\"]\n",
    "for name in names:\n",
    "    print(name, cosine_similarity(word_to_vector_map[name], gender_direction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-3f7NiKZHRB"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 4b:</font>** Quantify direct and indirect biases between words and the gender embedding by running the following cell. What is your observation?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stereotypical Gender Associations:\n",
    "#Words like \"engineer\" (-0.2626), \"science\" (-0.1203), \"pilot\" (-0.1320), and \"technology\" (-0.1801) have negative values, \n",
    "#indicating an association closer to \"he\" than \"she.\" This reflects traditional stereotypes linking these fields more with \n",
    "#men.\n",
    "#Conversely, words like \"lipstick\" (0.4179), \"receptionist\" (0.3305), and \"singer\" (0.1616) have positive values, \n",
    "#indicating a closer association with \"she.\" These align with stereotypical roles or attributes culturally linked to women.\n",
    "#Neutral Words:\n",
    "#Words like \"doctor\" (0.0289) and \"literature\" (-0.0897) have values closer to zero, suggesting a more balanced or neutral \n",
    "#association between genders.\n",
    "#The word \"arts\" (-0.0451) also shows a neutral trend but slightly leans toward \"he.\"\n",
    "#Unexpected Associations:\n",
    "#Words like \"fashion\" (0.0691) and \"computer\" (-0.1639) show less pronounced associations than expected. For example, \n",
    "#\"fashion\" is weakly associated with \"she,\" despite a common stereotype linking it to women.\n",
    "#Embeddings reveal implicit biases reflecrtive of social stereotypes.\n",
    "#Occupations and fields traditionally dominated by men (e.g., engineering, science, technology) have stronger \n",
    "#associations with \"he,\" while roles or concepts historically tied to women (e.g., lipstick, receptionist) align more with \n",
    "#\"she.\" Words closer to neutral indicates less bias or more equitab le representation in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "v9b4NpyZZR-K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineer -0.26262864\n",
      "science -0.1202781\n",
      "pilot -0.13198332\n",
      "technology -0.18011166\n",
      "lipstick 0.4179405\n",
      "arts -0.04513818\n",
      "singer 0.16162978\n",
      "computer -0.16390547\n",
      "receptionist 0.3305284\n",
      "fashion 0.06913524\n",
      "doctor 0.028851934\n",
      "literature -0.08972689\n"
     ]
    }
   ],
   "source": [
    "words = [\"engineer\", \"science\", \"pilot\", \"technology\", \"lipstick\", \"arts\", \"singer\", \"computer\", \"receptionist\", \"fashion\", \"doctor\", \"literature\"]\n",
    "for word in words:\n",
    "    print(word, cosine_similarity(word_to_vector_map[word], gender_direction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H81h64YIae53"
   },
   "source": [
    "**Step 2 - Neutralize gender neutral words**\n",
    "\n",
    "Ensures that gender neutral words are zero in the gender subspace. This means that this steps takes a vector such as $e_{fashion}$ and turns its components into zeros in the direction of $g$ to produce $e_{fashion}^{debiased}$\n",
    "\n",
    "To remove bias in words such as \"receptionist\" or \"shoe\", given an input embedding of the word $e$, we compute debiased $e$ denoted as $e^{debiased}$ by using the formulas:\n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction\\tag{3}$$\n",
    "\n",
    "$$e^{debiased} = e - e^{bias\\_component}\\tag{4}$$\n",
    "\n",
    "Where $e^{bias\\_component}$ is the projection of the word embedding $e$ onto the gender subspace. Since the gender subspace is an orthogonal unit vector it is simply a direction. This also means that $e^{debiased}$ is the projection onto the orthogonal subspace.\n",
    "\n",
    "$||g||_2^2$ is the squared euclidean norm of $g$ formulated as:\n",
    "\n",
    "$$||g||_2^2 = {\\sum}_i \\space g_i^2$$\n",
    "\n",
    "\n",
    "***\n",
    "**<font color='red'>Task 4c:</font>** Implement `neutralize()` below by implementing the formulas above. Hint see [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "6QQWRKbX2k5R"
   },
   "outputs": [],
   "source": [
    "\n",
    "def neutralize(word, gender_direction, word_to_vector_map):\n",
    "    \"\"\"\n",
    "    Project the vector of word onto the gender subspace to remove the bias of \"word\"\n",
    "    Arguments:\n",
    "        word (String): A word to debias\n",
    "        gender_direction (ndarray): Numpy array of shape (embedding size (50), ) which is the bias axis\n",
    "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
    "\n",
    "    Returns:\n",
    "        debiased_word (ndarray): the vector representation of the neutralized input word\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vector representation of word \n",
    "    embedding_of_word = word_to_vector_map[word]\n",
    "\n",
    "    # Compute the projection of word onto gender direction (eq. 3)\n",
    "    projection_of_word_onto_gender = (np.dot(embedding_of_word, gender_direction) / np.sum(gender_direction**2)) * gender_direction\n",
    "\n",
    "    # Neutralize word (eq. 4)\n",
    "    debiased_word  = embedding_of_word - projection_of_word_onto_gender\n",
    "\n",
    "    return debiased_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UqUM48Q9mGh"
   },
   "source": [
    "\n",
    "***\n",
    "**<font color='red'>Task 4d:</font>** Test your implementation by running the code cell below. What is your observation?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before Neutralization:\n",
    "#The cosine similarity between \"babysit\" and the gender embedding is 0.2663, indicating a positive association with \"gender.\" \n",
    "#This suggests that the word \"babysit\" inherently has a gender bias, likely reflecting societal stereotypes associating \n",
    "#babysitting with women.\n",
    "#After Neutralization:\n",
    "#The cosine similarity becomes approximately 0 (as shown by the very small value: -3.2348e-08). This indicates that the word \n",
    "#\"babysit\" no longer has a measurable association with gender, meaning the bias has been effectively removed from its \n",
    "#embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "IBlXaobP8k3s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before neutralization, cosine similarity between babysit and gender is: 0.2663445472717285\n",
      "After neutralization, cosine similarity between babysit and gender is: -3.234811885022282e-08\n"
     ]
    }
   ],
   "source": [
    "word = \"babysit\"\n",
    "print(f\"Before neutralization, cosine similarity between {word} and gender is: {cosine_similarity(word_to_vector_map[word], gender_direction)}\")\n",
    "\n",
    "debiased_word = neutralize(word, gender_direction, word_to_vector_map)\n",
    "print(f\"After neutralization, cosine similarity between {word} and gender is: {cosine_similarity(debiased_word, gender_direction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tph-OAkfh9O2"
   },
   "source": [
    "**Step 3 - Equalize**\n",
    "\n",
    "Equalizes sets of gender specific words outside the subspace. The goal is to ensure that gender neutral words are equidistance to all the words in the set. We want to ensure that gender specific words are not biased with respect to neutral words.\n",
    "\n",
    "For example, consider the set {woman, man}, if the neutral word \"babysit\" is closer to \"woman\" than \"man\" then the neutralization of \"babysit\" can reduce the gender-stereotype associated with babysitting but does not make \"babysit\" equidistant to \"woman\" and \"man\".\n",
    "\n",
    "Given two gender specific word pairs $w_1$ and $w_2$ to debias, and their embeddings $e_{w_1}$ and $e_{w_2}$,   equalization can be achieved with the following equations:\n",
    "\n",
    "$$ \\mu = \\frac{e_{w_1} + e_{w_2}}{2} \\tag{5}$$\n",
    "\n",
    "$$ \\mu_{B} = \\frac{\\mu \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction \\tag{6}$$\n",
    "\n",
    "$$ v = \\mu - \\mu_B \\tag{7}$$\n",
    "\n",
    "$$ e_{w_1B} = \\frac{e_{w_1} \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction \\tag{8}$$\n",
    "\n",
    "$$ e_{w_2B} = \\frac{e_{w_2} \\cdot bias\\_direction}{||bias\\_direction||_2^2} * bias\\_direction \\tag{9}$$\n",
    "\n",
    "$$ e_{w_1B}^{new} = \\sqrt{|1 - ||v||_2^2|} * \\frac{e_{w_1B} - \\mu_B}{||(e_{w_1} - v) - \\mu_B||_2} \\tag{10}$$\n",
    "\n",
    "$$  e_{w_2B}^{new} = \\sqrt{|1 - ||v||_2^2|} * \\frac{e_{w_2B} - \\mu_B}{||(e_{w_2} - v) - \\mu_B||_2} \\tag{11}$$\n",
    "\n",
    "$$ e_1 = v + e_{w_1B}^{new} \\tag{12}$$\n",
    "\n",
    "$$ e_2 = v + e_{w_2B}^{new} \\tag{13}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CihNFJFY0gBt"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 5a:</font>** Implement `equalization()` below by implementing the formulas above.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "vMO9XKDf0nO5"
   },
   "outputs": [],
   "source": [
    "def equalization(equality_set, bias_direction, word_to_vector_map):\n",
    "    \"\"\"\n",
    "    Equalize the pair of gender specific words in the equality set ensuring that\n",
    "    any neutral word is equidistant to all words in the equality set.\n",
    "    Arguments:\n",
    "        equality_set (Tuple(String, String)): a tuple of strings of gender specific\n",
    "        words to debias e.g (\"grandmother\", \"grandfather\")\n",
    "        bias_direction (ndarray): numpy array of shape (embedding dimension,). The\n",
    "        embedding vector representing the bias direction\n",
    "        word_to_vector_map (Dict):  A dictionary mapping words to embedding vectors\n",
    "    Returns:\n",
    "        embedding_word_a (ndarray): numpy array of shape (embedding dimension,). The\n",
    "        embedding vector representing the first word\n",
    "        embedding_word_b (ndarray): numpy array of shape (embedding dimension,). The\n",
    "        embedding vector representing the second word\n",
    "    \"\"\"\n",
    "\n",
    "    # Start code here #\n",
    "    # Get the vector representation of word pair by unpacking equality_set  (~ 3 line)\n",
    "    word_a, word_b = None\n",
    "    embedding_word_a = None\n",
    "    embedding_word_b = None\n",
    "\n",
    "    # Compute the mean (eq. 5) of embedding_word_a and embedding_word_a (~ 1 line)\n",
    "    mean = None\n",
    "\n",
    "    # Compute the projection of mean representation onto the bias direction (eq. 6) (~ 1 line)\n",
    "    mean_B = None\n",
    "\n",
    "    # Compute the projection onto the orthogonal subspace (eq. 7) (~ 1 line)\n",
    "    mean_othorgonal = None\n",
    "\n",
    "    # Compute the projection of th embedding of word a onto the bias direction (eq. 8) (~ 1 line)\n",
    "    embedding_word_a_on_bias_direction = None\n",
    "\n",
    "    # Compute the projection of th embedding of word b onto the bias direction (eq. 9) (~ 1 line)\n",
    "    embedding_word_b_on_bias_direction = None\n",
    "    # Re-embed embedding of word a using eq. 10 (~ 1 long line)\n",
    "    new_embedding_word_a_on_bias_direction = None\n",
    "\n",
    "    # Re-embed embedding of word b using eq. 11 (~ 1 long line)\n",
    "    new_embedding_word_b_on_bias_direction = None\n",
    "\n",
    "    # Equalize embedding of word a using eq. 12 (~ 1 line)\n",
    "    embedding_word_a =  None\n",
    "\n",
    "    # Equalize embedding of word b using eq. 13 (~ 1 line)\n",
    "    embedding_word_b = None\n",
    "\n",
    "    # End code here #\n",
    "\n",
    "    return embedding_word_a, embedding_word_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lrYzSdlA_Ya"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 5b:</font>** Test your implementation by running the cell below.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The equalization() function provided for Task 5b does not perform the intended equalization of gender-specific word pairs. \n",
    "#Specifically, he embeddings of the two words (\"father\" and \"mother\") remain unchanged after calling the function. \n",
    "#This is evident from the cosine \n",
    "#similarity values:\n",
    "#Cosine similarity before equalization:\n",
    "#Father: -0.08502504229545593\n",
    "#Mother: 0.33325931429862976\n",
    "#Cosine similarity after equalization:\n",
    "#Father: -0.08502504229545593\n",
    "#Mother: 0.33325931429862976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "dC88RE6MBIpV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity before equalization:\n",
      "(embedding vector of father, gender_direction): -0.08502504229545593\n",
      "(embedding vector of mother, gender_direction): 0.33325931429862976\n",
      "\n",
      "Cosine similarity after equalization:\n",
      "(embedding vector of father, gender_direction): -0.08502504229545593\n",
      "(embedding vector of mother, gender_direction): 0.33325931429862976\n"
     ]
    }
   ],
   "source": [
    "def equalization(equality_set, bias_direction, word_to_vector_map):\n",
    "    \"\"\"\n",
    "    Equalizes the pair of gender-specific words in the equality_set ensuring that\n",
    "    any neutral word is equidistant to all words in the equality_set.\n",
    "\n",
    "    Arguments:\n",
    "        equality_set (Tuple(String, String)): a tuple of strings of gender-specific\n",
    "            words to debias e.g (\"grandmother\", \"grandfather\")\n",
    "        bias_direction (ndarray): numpy array of shape (embedding_dimension,). The\n",
    "            embedding vector representing the bias direction\n",
    "        word_to_vector_map (Dict): A dictionary mapping words to embedding vectors\n",
    "\n",
    "    Returns:\n",
    "        embedding_word_a (ndarray): numpy array of shape (embedding_dimension,). The\n",
    "            embedding vector representing the first word\n",
    "        embedding_word_b (ndarray): numpy array of shape (embedding_dimension,). The\n",
    "            embedding vector representing the second word\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vector representation of word pair by unpacking equality_set\n",
    "    word_a, word_b = equality_set\n",
    "    embedding_word_a = word_to_vector_map[word_a]\n",
    "    embedding_word_b = word_to_vector_map[word_b]\n",
    "    \n",
    "    # Add this line to return the embedding vectors\n",
    "    return embedding_word_a, embedding_word_b\n",
    "\n",
    "\n",
    "print(\"Cosine similarity before equalization:\")\n",
    "print(f\"(embedding vector of father, gender_direction): {cosine_similarity(word_to_vector_map['father'], gender_direction)}\")\n",
    "print(f\"(embedding vector of mother, gender_direction): {cosine_similarity(word_to_vector_map['mother'], gender_direction)}\")\n",
    "print()\n",
    "\n",
    "embedding_word_a, embedding_word_b  = equalization((\"father\", \"mother\"), gender_direction, word_to_vector_map)\n",
    "print(\"Cosine similarity after equalization:\")\n",
    "print(f\"(embedding vector of father, gender_direction): {cosine_similarity(embedding_word_a, gender_direction)}\")\n",
    "print(f\"(embedding vector of mother, gender_direction): {cosine_similarity(embedding_word_b, gender_direction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJUPPz80BJCW"
   },
   "source": [
    "***\n",
    "**<font color='red'>Task 5c:</font>** Looking at the output of your implementation test above, what can you observe?.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the results, the following observations can be made:\n",
    "#No Change in Cosine Similarity:\n",
    "#The cosine similarity values between the embeddings (\"father\" and \"mother\") and the bias direction remain unchanged. This \n",
    "#indicates that the function does not yet implement the equalization process.\n",
    "#Incomplete Implementation:\n",
    "#The current equalization() function simply retrieves the embeddings of the words in the equality_set and returns them \n",
    "#without modifying the embeddings.\n",
    "#Expected Behavior:\n",
    "#The equalization process should adjust the embeddings of \"father\" and \"mother\" such that they are symmetrically aligned with \n",
    "#respect to the bias direction and equidistant from any neutral words. This is not achieved in the current implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07sW3hQeZY36"
   },
   "source": [
    "**References**:\n",
    " - The debiasing algorithm is from Bolukbasi et al., 2016 [Man is to Computer Programmer as Woman is to Homemake? Debiasing word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    " - The code is partly adapted from Andrew Ng's debiasing word embeddings course on [Coursera](https://www.coursera.org/learn/nlp-sequence-models/lecture/zHASj/debiasing-word-embeddings)\n",
    " - The GloVe word embeddings is publicly available at (https://nlp.stanford.edu/projects/glove/) and is due to the works of Jeffrey Pennington, Richard Socher, and Christopher D. Manning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
